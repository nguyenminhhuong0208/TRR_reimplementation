import math
import re
import time
import os
import random
import pickle
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor

import google.generativeai as genai
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv

load_dotenv()

import tiktoken
tokenize = tiktoken.get_encoding('cl100k_base')

genai.configure(api_key = os.getenv("GOOGLE_API_KEY"))

# Initialize models globally once
model = ChatGoogleGenerativeAI(model = "gemini-2.0-flash-lite", temperature= 0.02)
time.sleep(1)
model_more_temperature = ChatGoogleGenerativeAI(model = "gemini-2.0-flash-lite", temperature= 0.1)
time.sleep(1)
model2 = ChatGoogleGenerativeAI(model = "gemini-2.5-pro-preview-05-06", temperature= 0.1)
time.sleep(1)

import pandas as pd
from dateutil.parser import isoparse
import networkx as nx
import json
import argparse

# ---------------------------
# Parameters and Setup
# ---------------------------
MAX_ITER = 3
PORTFOLIO_STOCKS = ["FPT", "SSI", "VCB", "VHM", "HPG", "GAS", "MSN", "MWG", "GVR", "VCG"]
PORTFOLIO_SECTOR = ["C√¥ng ngh·ªá", "Ch·ª©ng kho√°n", "Ng√¢n h√†ng", "B·∫•t ƒë·ªông s·∫£n", "V·∫≠t li·ªáu c∆° b·∫£n", "D·ªãch v·ª• H·∫° t·∫ßng", "Ti√™u d√πng c∆° b·∫£n", "B√°n l·∫ª", "Ch·∫ø bi·∫øn", "C√¥ng nghi·ªáp"]
# Maximum retries and base delay for exponential backoff
MAX_RETRIES = 5
BASE_DELAY = 30

# ---------------------------
# Prompt Templates
# ---------------------------
news_summarize_template = PromptTemplate.from_template("""
B·∫°n l√† m·ªôt chuy√™n gia t√≥m t·∫Øt tin t·ª©c kinh t·∫ø th·ªã tr∆∞·ªùng. 
D·ªØ li·ªáu ƒë·∫ßu v√†o g·ªìm c√°c b√†i b√°o trong ng√†y, m·ªói b√†i b√°o c√≥ ti√™u ƒë·ªÅ, m√¥ t·∫£ v√† ch·ªß ƒë·ªÅ. 
Nhi·ªám v·ª• c·ªßa b·∫°n l√† t√≥m t·∫Øt, k·∫øt h·ª£p v√† c√¥ ƒë·ªçng n·ªôi dung c·ªßa c√°c tin t·ª©c ƒë√≥ th√†nh 20 tin ch√≠nh, 
sao cho m·ªói tin t√≥m t·∫Øt ph·∫£n √°nh ƒë·∫ßy ƒë·ªß nh·ªØng ƒëi·ªÉm quan tr·ªçng c·ªßa b√†i b√°o g·ªëc, m·ªôt c√°ch ng·∫Øn g·ªçn v√† s√∫c t√≠ch, tr√™n c√πng m·ªôt d√≤ng.

Danh s√°ch b√†i b√°o:
{articles_list}

H√£y xu·∫•t ƒë·∫ßu ra theo ƒë·ªãnh d·∫°ng sau:
[Ch·ªß ƒë·ªÅ b√†i b√°o 1]: [Ti√™u ƒë·ªÅ] | [N·ªôi dung t√≥m t·∫Øt]
[Ch·ªß ƒë·ªÅ b√†i b√°o 2]: [Ti√™u ƒë·ªÅ] | [N·ªôi dung t√≥m t·∫Øt]
...
[Ch·ªß ƒë·ªÅ b√†i b√°o N]: [Ti√™u ƒë·ªÅ] | [N·ªôi dung t√≥m t·∫Øt]

V√≠ d·ª•:
(BƒÇT ƒê·∫¶U V√ç D·ª§)
Ch·ªß ƒë·ªÅ Th·ªã tr∆∞·ªùng:
M·ªôt trong nh·ªØng "c√° m·∫≠p" l·ªõn nh·∫•t tr√™n TTCK li√™n t·ª•c h√∫t th√™m v√†i ngh√¨n t·ª∑ v·ªën ngo·∫°i trong 4 th√°ng qua, mua h√†ng ch·ª•c tri·ªáu c·ªï phi·∫øu ng√¢n h√†ng, HPG, DXG...","ƒêi ng∆∞·ª£c v·ªõi xu h∆∞·ªõng r√∫t r√≤ng c·ªßa ph·∫ßn l·ªõn c√°c qu·ªπ tr√™n th·ªã tr∆∞·ªùng, VFMVSF ng√†y c√†ng thu h·∫πp kho·∫£ng c√°ch v·ªõi qu·ªπ c√≥ quy m√¥ t√†i s·∫£n ƒë·ª©ng ƒë·∫ßu th·ªã tr∆∞·ªùng l√† VN DIAMOND ETF. C·∫£ 2 ƒë·ªÅu thu·ªôc qu·∫£n l√Ω c·ªßa Dragon Capital.

Ch·ªß ƒë·ªÅ Th·∫ø gi·ªõi:
Ch√≠nh ph·ªß Cuba √°p gi√° tr·∫ßn t·∫°m th·ªùi v·ªõi n√¥ng s·∫£n,Cuba √°p gi√° tr·∫ßn t·∫°m th·ªùi ƒë·ªëi v·ªõi c√°c s·∫£n ph·∫©m n√¥ng s·∫£n thi·∫øt y·∫øu nh·∫±m ki·ªÅm ch·∫ø l·∫°m ph√°t v√† kh·ªßng ho·∫£ng kinh t·∫ø nghi√™m tr·ªçng.

Ch·ªß ƒë·ªÅ Th·∫ø gi·ªõi:
C√°c ng√¢n h√†ng M·ªπ lo ng·∫°i v·ªÅ nguy c∆° suy tho√°i n·ªÅn kinh t·∫ø," C√°c ng√¢n h√†ng M·ªπ ƒëang lo ng·∫°i v·ªÅ nguy c∆° suy tho√°i kinh t·∫ø, theo h√£ng tin Bloomberg. N·ªói lo n√†y xu·∫•t ph√°t t·ª´ c√°c m·ª©c thu·∫ø quan m·ªõi v√† ch·ªâ s·ªë kinh t·∫ø kh√¥ng m·∫•y kh·∫£ quan.

Ch·ªß ƒë·ªÅ Th·∫ø gi·ªõi:
Chi·∫øn l∆∞·ª£c c·∫°nh tranh c·ªßa ch√¢u √Çu trong ng√†nh c√¥ng nghi·ªáp xanh,B√°o c√°o v·ªÅ kh·∫£ nƒÉng c·∫°nh tranh cho r·∫±ng ch√¢u √Çu n√™n t·∫≠p trung ƒë·∫ßu t∆∞ v√†o c√°c c√¥ng ngh·ªá m·ªõi n·ªïi, ch·∫≥ng h·∫°n nh∆∞ hydro v√† pin, thay v√¨ c·ªë g·∫Øng c·∫°nh tranh v·ªõi Trung Qu·ªëc trong s·∫£n xu·∫•t t·∫•m pin M·∫∑t tr·ªùi.

Ch·ªß ƒë·ªÅ Th·∫ø gi·ªõi:
C∆° h·ªôi cho ·∫§n ƒê·ªô," B√°o Deccan Herald v·ª´a ƒëƒÉng b√†i ph√¢n t√≠ch c·ªßa chuy√™n gia kinh t·∫ø Ajit Ranade, ƒë√°nh gi√° v·ªÅ t√°c ƒë·ªông v√† c∆° h·ªôi ƒë·ªëi v·ªõi ·∫§n ƒê·ªô t·ª´ nh·ªØng quy·∫øt ƒë·ªãnh m·ªõi nh·∫•t c·ªßa T·ªïng th·ªëng M·ªπ Donald Trump v·ªÅ thu·∫ø quan."

Ch·ªß ƒë·ªÅ H√†ng h√≥a:
Gi√° v√†ng h√¥m nay (9-3): Quay ƒë·∫ßu gi·∫£m,"Gi√° v√†ng h√¥m nay (9-3): Gi√° v√†ng trong n∆∞·ªõc h√¥m nay gi·∫£m nh·∫π, v√†ng mi·∫øng SJC nhi·ªÅu th∆∞∆°ng hi·ªáu gi·∫£m 200.000 ƒë·ªìng/l∆∞·ª£ng."

Ch·ªß ƒë·ªÅ H√†ng h√≥a:
Gi√° xƒÉng d·∫ßu h√¥m nay (9-3): Tu·∫ßn gi·∫£m m·∫°nh, c√≥ th·ªùi ƒëi·ªÉm b·ªè m·ªëc 70 USD/th√πng,"Gi√° xƒÉng d·∫ßu th·∫ø gi·ªõi l·∫≠p hat-trick gi·∫£m tu·∫ßn. ƒê√°ng ch√∫ √Ω l√† trong tu·∫ßn, gi√° d·∫ßu c√≥ th·ªùi ƒëi·ªÉm tr∆∞·ª£t xa m·ªëc 70 USD/th√πng."

Ch·ªß ƒë·ªÅ T√†i ch√≠nh:
T·ª∑ gi√° USD h√¥m nay (9-3): ƒê·ªìng USD lao d·ªëc k·ª∑ l·ª•c,"T·ª∑ gi√° USD h√¥m nay: R·∫°ng s√°ng 9-3, Ng√¢n h√†ng Nh√† n∆∞·ªõc c√¥ng b·ªë t·ª∑ gi√° trung t√¢m c·ªßa ƒë·ªìng Vi·ªát Nam v·ªõi USD tƒÉng tu·∫ßn 12 ƒë·ªìng, hi·ªán ·ªü m·ª©c 24.738 ƒë·ªìng."

Ch·ªß ƒë·ªÅ H√†ng h√≥a:
B·∫£n tin n√¥ng s·∫£n h√¥m nay (9-3): Gi√° h·ªì ti√™u ·ªïn ƒë·ªãnh m·ª©c cao,B·∫£n tin n√¥ng s·∫£n h√¥m nay (9-3) ghi nh·∫≠n gi√° h·ªì ti√™u ·ªïn ƒë·ªãnh m·ª©c cao; gi√° c√† ph√™ ti·∫øp t·ª•c gi·∫£m nh·∫π.

Ch·ªß ƒë·ªÅ T√†i ch√≠nh:
Standard Chartered ƒëi·ªÅu ch·ªânh d·ª± b√°o t·ª∑ gi√° USD/VND, n√¢ng m·ª©c gi·ªØa nƒÉm l√™n 26.000 ƒë·ªìng/USD v√† cu·ªëi nƒÉm 2025 l√™n 25.700 ƒë·ªìng/USD, ph·∫£n √°nh s·ª©c √©p t·ª´ bi·∫øn ƒë·ªông kinh t·∫ø to√†n c·∫ßu v√† khu v·ª±c.

Ch·ªß ƒë·ªÅ T√†i ch√≠nh:
Ng√¢n h√†ng ACB Vi·ªát Nam d·ª± ki·∫øn g√≥p th√™m 1.000 t·ª∑ ƒë·ªìng ƒë·ªÉ tƒÉng v·ªën ƒëi·ªÅu l·ªá cho ACBS l√™n m·ª©c 11.000 t·ª∑ ƒë·ªìng.

Ch·ªß ƒë·ªÅ T√†i ch√≠nh:
B·∫•t ·ªïn thu·∫ø quan ti·∫øp t·ª•c th√∫c ƒë·∫©y nhu c·∫ßu tr√∫ ·∫©n an to√†n b·∫±ng v√†ng, Gi√° v√†ng tƒÉng nh·∫π t·∫°i ch√¢u √Å do b·∫•t ·ªïn v·ªÅ ch√≠nh s√°ch thu·∫ø quan c·ªßa M·ªπ ti·∫øp t·ª•c th√∫c ƒë·∫©y nhu c·∫ßu tr√∫ ·∫©n an to√†n tr∆∞·ªõc nh·ªØng lo ng·∫°i v·ªÅ nguy c∆° kinh t·∫ø M·ªπ suy y·∫øu v√† l·∫°m ph√°t gia tƒÉng.

Ch·ªß ƒë·ªÅ B·∫•t ƒë·ªông s·∫£n:
C·∫£ n∆∞·ªõc d·ª± ki·∫øn gi·∫£m t·ª´ 10.500 ƒë∆°n v·ªã c·∫•p x√£ xu·ªëng kho·∫£ng 2.500 ƒë∆°n v·ªã,"Theo Ph√≥ th·ªß t∆∞·ªõng Nguy·ªÖn H√≤a B√¨nh, s·ªë l∆∞·ª£ng ƒë∆°n v·ªã h√†nh ch√≠nh c·∫•p x√£ tr√™n to√†n qu·ªëc d·ª± ki·∫øn s·∫Ω gi·∫£m t·ª´ h∆°n 10.500 xu·ªëng c√≤n 2.500 sau khi s√°p nh·∫≠p.


T√≥m t·∫Øt tin t·ª©c trong ng√†y:
T√†i ch√≠nh: "C√° m·∫≠p" l·ªõn tr√™n TTCK li√™n t·ª•c h√∫t th√™m v√†i ngh√¨n t·ª∑ v·ªën ngo·∫°i 4 th√°ng qua | M·ªôt trong nh·ªØng "c√° m·∫≠p" l·ªõn tr√™n TTCK li√™n t·ª•c h√∫t th√™m v√†i ngh√¨n t·ª∑ v·ªën ngo·∫°i trong 4 th√°ng qua v√† mua h√†ng ch·ª•c tri·ªáu c·ªï phi·∫øu ng√¢n h√†ng, HPG, DXG. ƒê·ªìng th·ªùi, VFMVSF ng√†y c√†ng thu h·∫πp kho·∫£ng c√°ch v·ªõi VN DIAMOND ETF - c·∫£ hai ƒë·ªÅu do Dragon Capital qu·∫£n l√Ω, ph·∫£n √°nh xu h∆∞·ªõng ƒë·∫ßu t∆∞ m·∫°nh m·∫Ω c·ªßa c√°c qu·ªπ chi·∫øn l∆∞·ª£c.

Th·∫ø gi·ªõi: Nguy c∆° suy tho√°i ƒëe d·ªça M·ªπ, v√† c∆° h·ªôi m·ªü ra cho ·∫§n ƒê·ªô | C√°c ng√¢n h√†ng M·ªπ ƒëang lo ng·∫°i nguy c∆° suy tho√°i kinh t·∫ø khi ƒë·ªëi m·∫∑t v·ªõi c√°c m·ª©c thu·∫ø quan m·ªõi v√† ch·ªâ s·ªë kinh t·∫ø kh√¥ng kh·∫£ quan, theo th√¥ng tin t·ª´ Bloomberg, cho th·∫•y s·ª± b·∫•t ·ªïn c√≥ th·ªÉ lan r·ªông trong n·ªÅn kinh t·∫ø Hoa K·ª≥, nh∆∞ng l√† c∆° h·ªôi cho ·∫§n ƒê·ªô khi c√°c quy·∫øt ƒë·ªãnh m·ªõi c·ªßa T·ªïng th·ªëng M·ªπ Donald Trump v·ªÅ thu·∫ø quan c√≥ th·ªÉ t·∫°o ra nh·ªØng t√°c ƒë·ªông t√≠ch c·ª±c ƒë·ªëi v·ªõi n·ªÅn kinh t·∫ø ·∫§n ƒê·ªô,

Th·∫ø gi·ªõi: Cuba √°p gi√° tr·∫ßn n√¥ng s·∫£n ƒë·ªÉ h·∫° nhi·ªát l·∫°m ph√°t | Ch√≠nh ph·ªß Cuba √°p gi√° tr·∫ßn t·∫°m th·ªùi cho c√°c s·∫£n ph·∫©m n√¥ng s·∫£n thi·∫øt y·∫øu nh·∫±m ki·ªÅm ch·∫ø l·∫°m ph√°t v√† ngƒÉn ch·∫∑n kh·ªßng ho·∫£ng kinh t·∫ø, t·∫°o ra m·ªôt bi·ªán ph√°p t·∫°m th·ªùi ƒë·ªÉ ·ªïn ƒë·ªãnh th·ªã tr∆∞·ªùng n·ªôi ƒë·ªãa trong b·ªëi c·∫£nh kinh t·∫ø kh√≥ khƒÉn.

Th·∫ø gi·ªõi: EU khuy·∫øn ngh·ªã ƒë·∫ßu t∆∞ v√†o c√¥ng ngh·ªá xanh thay v√¨ c·∫°nh tranh pin m·∫∑t tr·ªùi v·ªõi Trung Qu·ªëc | B√°o c√°o c·∫°nh tranh c·ªßa ch√¢u √Çu khuy·∫øn ngh·ªã n√™n t·∫≠p trung ƒë·∫ßu t∆∞ v√†o c√°c c√¥ng ngh·ªá m·ªõi n·ªïi nh∆∞ hydro v√† pin thay v√¨ c·ªë g·∫Øng c·∫°nh tranh tr·ª±c ti·∫øp v·ªõi Trung Qu·ªëc trong s·∫£n xu·∫•t t·∫•m pin m·∫∑t tr·ªùi, nh·∫±m x√¢y d·ª±ng n·ªÅn c√¥ng nghi·ªáp xanh b·ªÅn v·ªØng.

H√†ng h√≥a: Gi√° v√†ng v√† xƒÉng d·∫ßu bi·∫øn ƒë·ªông m·∫°nh tr∆∞·ªõc s·ª©c √©p thu·∫ø quan | Gi√° v√†ng trong n∆∞·ªõc gi·∫£m nh·∫π trong phi√™n giao d·ªãch ng√†y 9-3, v·ªõi v√†ng mi·∫øng SJC nhi·ªÅu th∆∞∆°ng hi·ªáu gi·∫£m kho·∫£ng 200.000 ƒë·ªìng/l∆∞·ª£ng, do xu·∫•t hi·ªán nhu c·∫ßu tr√∫ ·∫©n an to√†n b·∫±ng v√†ng t·∫°i Ch√¢u √Å tr∆∞·ªõc s·ª©c √©p thu·∫ø quan. Gi√° xƒÉng d·∫ßu th·∫ø gi·ªõi c√≥ ƒë·ª£t gi·∫£m m·∫°nh trong tu·∫ßn qua, tr∆∞·ª£t xu·ªëng d∆∞·ªõi m·ªëc 70 USD/th√πng, cho th·∫•y s·ª± bi·∫øn ƒë·ªông m·∫°nh m·∫Ω c·ªßa th·ªã tr∆∞·ªùng kim lo·∫°i qu√Ω.

T√†i ch√≠nh: T·ª∑ gi√° USD/VND bi·∫øn ƒë·ªông m·∫°nh, d·ª± b√°o tƒÉng ƒë·∫øn cu·ªëi nƒÉm 2025 | T·ª∑ gi√° USD lao d·ªëc k·ª∑ l·ª•c trong phi√™n giao d·ªãch s√°ng 9-3 khi Ng√¢n h√†ng Nh√† n∆∞·ªõc tƒÉng 12 ƒë·ªìng, USD/VND ƒë∆∞·ª£c ƒë·ªãnh gi√° ·ªü m·ª©c 24.738 ƒë·ªìng. Tuy nhi√™n Standard Chartered ƒë√£ ƒëi·ªÅu ch·ªânh d·ª± b√°o t·ª∑ gi√° USD/VND gi·ªØa nƒÉm l√™n 26.000 ƒë·ªìng/USD v√† cu·ªëi nƒÉm 2025 l√™n 25.700 ƒë·ªìng/USD, ph·∫£n √°nh s·ª©c √©p t·ª´ bi·∫øn ƒë·ªông kinh t·∫ø to√†n c·∫ßu v√† khu v·ª±c.

H√†ng h√≥a: Gi√° n√¥ng s·∫£n Vi·ªát Nam ·ªïn ƒë·ªãnh | Gi√° n√¥ng s·∫£n ·ªïn ƒë·ªãnh, trong ƒë√≥ h·ªì ti√™u gi·ªØ m·ª©c cao, gi√° c√† ph√™ gi·∫£m nh·∫π.

B·∫•t ƒë·ªông s·∫£n: S√°p nh·∫≠p h√†nh ch√≠nh, tinh gi·∫£n h·ªá th·ªëng qu·∫£n l√Ω | Theo Ph√≥ th·ªß t∆∞·ªõng Nguy·ªÖn H√≤a B√¨nh, sau qu√° tr√¨nh s√°p nh·∫≠p h√†nh ch√≠nh, s·ªë l∆∞·ª£ng ƒë∆°n v·ªã c·∫•p x√£ tr√™n c·∫£ n∆∞·ªõc d·ª± ki·∫øn s·∫Ω gi·∫£m t·ª´ h∆°n 10.500 xu·ªëng c√≤n kho·∫£ng 2.500, t·∫°o n√™n c∆° c·∫•u h√†nh ch√≠nh h·ª£p l√Ω h∆°n v√† ·∫£nh h∆∞·ªüng ƒë·∫øn ch√≠nh s√°ch qu·∫£n l√Ω ƒë·ªãa ph∆∞∆°ng.

(K·∫æT TH√öC V√ç D·ª§)

L∆∞u √Ω:
- ∆Øu ti√™n nh·ªØng b√†i b√°o c√≥ chung ch·ªß ƒë·ªÅ, ho·∫∑c c√≥ li√™n quan, g·ªôp v√†o th√†nh m·ªôt b√†i b√°o duy nh·∫•t.
- M·ªói b√†i t√≥m t·∫Øt ph·∫£i g·ªìm nh·ªØng ƒëi·ªÉm ch√≠nh v·ªÅ ti√™u ƒë·ªÅ v√† m√¥ t·∫£. M·ªói b√†i b√°o ƒë·ªÅu ƒë∆∞·ª£c ghi tr√™n m·ªôt d√≤ng. Ch√∫ √Ω trong b√†i t√≥m t·∫Øt kh√¥ng ƒë∆∞·ª£c c√≥ d·∫•u xu·ªëng d√≤ng hay d·∫•u hai ch·∫•m :.
- T·ªïng s·ªë tin sau khi t√≥m t·∫Øt l√† 20 tin.
- Lu√¥n ch·ª©a to√†n b·ªô s·ªë li·ªáu ƒë∆∞·ª£c ghi trong b√†i b√°o.
- Kh√¥ng ƒë∆∞·ª£c t·ª± t·∫°o th√™m tin t·ª©c m·ªõi. Kh√¥ng d√πng s·ªë li·ªáu ngo√†i b√†i b√°o.
- N·∫øu b√†i b√°o n√†o thu·ªôc danh m·ª•c c·ªï phi·∫øu sau: {portfolio}, h√£y ghi r√µ t√™n c·ªï phi·∫øu, v√† d√†nh ri√™ng m·ªôt tin cho n√≥.
- Kh√¥ng ∆∞u ti√™n nh·ªØng b√†i b√°o ch·ªâ n√≥i v·ªÅ m·ªôt c√¥ng ty c·ª• th·ªÉ ·ªü Vi·ªát Nam, v√† c√¥ng ty ƒë√≥ kh√¥ng thu·ªôc danh m·ª•c c·ªï phi·∫øu.

T√≥m t·∫Øt tin t·ª©c trong ng√†y:
""")

entity_extraction_template = PromptTemplate.from_template("""B·∫°n ƒëang l√†m vi·ªác d∆∞·ªõi b·ªëi c·∫£nh ph√¢n t√≠ch kinh t·∫ø. 
B·∫°n ƒë∆∞·ª£c cho m·ªôt ho·∫∑c nhi·ªÅu b√†i b√°o, bao g·ªìm t·ª±a ƒë·ªÅ v√† m√¥ t·∫£ ng·∫Øn g·ªçn v·ªÅ b√†i b√°o ƒë√≥, ngo√†i ra b·∫°n c√≥
th√¥ng tin v·ªÅ ng√†y xu·∫•t b·∫£n c·ªßa b√†i b√°o, v√† lo·∫°i ch·ªß ƒë·ªÅ m√† b√†i b√°o ƒëang ƒë·ªÅ c·∫≠p t·ªõi.

H·∫°n ch·∫ø t·∫°o m·ªõi m·ªôt th·ª±c th·ªÉ, ch·ªâ t·∫°o li√™n k·∫øt t·ªõi 5 th·ª±c th·ªÉ. Lu√¥n ∆∞u ti√™n li√™n k·∫øt v·ªõi c√°c th·ª±c th·ªÉ ƒë√£ c√≥: {existing_entities}

B·∫°n c·∫ßn ph√¢n t√≠ch b√†i b√°o, ƒë∆∞a ra t√™n c·ªßa nh·ªØng th·ª±c th·ªÉ (v√≠ d·ª• nh∆∞ c·ªï phi·∫øu, ng√†nh ngh·ªÅ, c√¥ng ty, qu·ªëc gia, t·ªânh th√†nh...)
s·∫Ω b·ªã ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp b·ªüi th√¥ng tin c·ªßa b√†i b√°o, theo h∆∞·ªõng t√≠ch c·ª±c ho·∫∑c ti√™u c·ª±c.

V·ªõi m·ªói th·ª±c th·ªÉ, ·ªü ph·∫ßn T√™n th·ª±c th·ªÉ, h·∫°n ch·∫ø d√πng d·∫•u ch·∫•m, g·∫°ch ngang, d·∫•u v√† &, d·∫•u ch·∫•m ph·∫©y ;. V√† c·∫ßn ghi th√™m qu·ªëc gia, ƒë·ªãa ph∆∞∆°ng c·ª• th·ªÉ v√† ng√†nh ngh·ªÅ c·ªßa n√≥ (n·∫øu c√≥).
T√™n ch·ªâ n√≥i t·ªõi m·ªôt th·ª±c th·ªÉ duy nh·∫•t. Ph·∫ßn T√™n kh√¥ng ƒë∆∞·ª£c qu√° ph·ª©c t·∫°p, ƒë∆°n gi·∫£n nh·∫•t c√≥ th·ªÉ.
N·∫øu th·ª±c th·ªÉ n√†o thu·ªôc danh m·ª•c c·ªï phi·∫øu sau: {portfolio}, h√£y ghi r√µ t√™n c·ªï phi·∫øu.
V√≠ d·ª•: SSI-Ch·ª©ng kho√°n; Ng√†nh c√¥ng nghi·ªáp Vi·ªát Nam; Ng∆∞·ªùi d√πng M·ªπ; Ng√†nh th√©p Ch√¢u √Å; Ng√†nh du l·ªãch H·∫° Long, ...

Ghi nh·ªõ, H·∫°n ch·∫ø t·∫°o m·ªõi m·ªôt th·ª±c th·ªÉ, ch·ªâ t·∫°o li√™n k·∫øt t·ªõi 5 th·ª±c th·ªÉ. Lu√¥n c·ªë li√™n k·∫øt v·ªõi c√°c th·ª±c th·ªÉ ƒë√£ c√≥.

Ph·∫ßn gi·∫£i th√≠ch m·ªói th·ª±c th·ªÉ, b·∫Øt bu·ªôc ƒë√°nh gi√° s·ªë li·ªáu ƒë∆∞·ª£c ghi, nhi·ªÅu ho·∫∑c √≠t, tƒÉng ho·∫∑c gi·∫£m, g·∫•p bao nhi√™u l·∫ßn, ...
C·∫ßn c·ªë g·∫Øng li√™n k·∫øt v·ªõi nhi·ªÅu th·ª±c th·ªÉ kh√°c. Tuy nhi√™n kh√¥ng suy ngo√†i ph·∫°m vi b√†i b√°o. Kh√¥ng t·ª± ch√®n s·ªë li·ªáu ngo√†i b√†i b√°o.
Kh√¥ng d√πng d·∫•u hai ch·∫•m trong ph·∫ßn gi·∫£i th√≠ch, ch·ªâ d√πng hai ch·∫•m : ƒë·ªÉ t√°ch gi·ªØa T√™n th·ª±c th·ªÉ v√† ph·∫ßn gi·∫£i th√≠ch.
                                                          
ƒê∆∞a ra theo ƒë·ªãnh d·∫°ng sau:
[[POSITIVE]]
[Entity 1]: [Explanation]
...
[Entity N]: [Explanation]

[[NEGATIVE]]
[Entity A]: [Explanation]
..
[Entity Z]: [Explanation]
                                                          
M·ªôt v√≠ d·ª• cho b√†i b√°o:

(B·∫ÆT ƒê·∫¶U V√ç D·ª§)

Ng√†y ƒëƒÉng: 2025-04-07T22:51:00+07:00
Lo·∫°i ch·ªß ƒë·ªÅ: Kinh t·∫ø
T·ª±a ƒë·ªÅ: N√¥ÃÉ l∆∞Ã£c hi·ªán th·ª±c h√≥a muÃ£c ti√™u th√¥ng tuy·∫øn cao t√¥ÃÅc t·ª´ Cao B·∫±ng ƒë·∫øn C√† Mau 

M√¥ t·∫£: Nh·∫±m ho√†n th√†nh m·ª•c ti√™u ƒë·∫øn nƒÉm 2025 c·∫£ n∆∞·ªõc c√≥ tr√™n 3.000 km ƒë∆∞·ªùng cao t·ªëc, B·ªô X√¢y d·ª±ng, c√°c ƒë·ªãa ph∆∞∆°ng v√† doanh nghi·ªáp ƒëang tri·ªÉn khai thi c√¥ng 28 d·ª± √°n/d·ª± √°n th√†nh ph·∫ßn v·ªõi t·ªïng chi·ªÅu d√†i kho·∫£ng 1.188 km. 
ƒê·∫øn nay, ti·∫øn ƒë·ªô ƒëa s·ªë c√°c d·ª± √°n b√°m s√°t k·∫ø ho·∫°ch, nhi·ªÅu d·ª± √°n ƒëƒÉng k√Ω ho√†n th√†nh th√¥ng tuy·∫øn trong nƒÉm 2025. C√≥ th·ªÉ n√≥i ng√†nh giao th√¥ng v·∫≠n t·∫£i ƒëang c·ªë g·∫Øng h·∫øt s·ª©c.

Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng:

[[POSITIVE]]
B·ªô X√¢y d·ª±ng Vi·ªát Nam: √Åp l·ª±c qu·∫£n l√Ω 28 d·ª± √°n v·ªõi t·ªïng chi·ªÅu d√†i 1188 km, nh·∫±m hi·ªán th·ª±c h√≥a m·ª•c ti√™u ƒë·∫°t 3000 km cao t·ªëc v√†o nƒÉm 2025. S·ªë l∆∞·ª£ng d·ª± √°n tƒÉng g·∫•p nhi·ªÅu l·∫ßn so v·ªõi giai ƒëo·∫°n tr∆∞·ªõc, ƒë√≤i h·ªèi ƒëi·ªÅu ph·ªëi ngu·ªìn l·ª±c v√† ki·ªÉm so√°t ti·∫øn ƒë·ªô ch·∫∑t ch·∫Ω h∆°n.
Ch√≠nh quy·ªÅn ƒë·ªãa ph∆∞∆°ng Vi·ªát Nam: Tr·ª±c ti·∫øp ph·ªëi h·ª£p tri·ªÉn khai c√°c d·ª± √°n t·∫°i t·ª´ng t·ªânh th√†nh. C·∫ßn n√¢ng cao nƒÉng l·ª±c qu·∫£n l√Ω v√† s·ª≠ d·ª•ng ng√¢n s√°ch c√¥ng hi·ªáu qu·∫£ ƒë·ªÉ ƒë·∫£m b·∫£o ti·∫øn ƒë·ªô thi c√¥ng theo k·∫ø ho·∫°ch chung qu·ªëc gia.
Doanh nghi·ªáp x√¢y d·ª±ng Vi·ªát Nam: ƒê∆∞·ª£c h∆∞·ªüng l·ª£i tr·ª±c ti·∫øp khi nh·∫≠n kh·ªëi l∆∞·ª£ng h·ª£p ƒë·ªìng thi c√¥ng l·ªõn. Doanh thu v√† nƒÉng l·ª±c thi c√¥ng c√≥ th·ªÉ tƒÉng nhanh h∆°n so v·ªõi c√°c giai ƒëo·∫°n tr∆∞·ªõc ƒë√¢y, nh·ªù nhu c·∫ßu ƒë·∫ßu t∆∞ h·∫° t·∫ßng tƒÉng m·∫°nh.
Ng√†nh giao th√¥ng v·∫≠n t·∫£i Vi·ªát Nam: C·∫£i thi·ªán h·∫° t·∫ßng cao t·ªëc gi√∫p r√∫t ng·∫Øn th·ªùi gian di chuy·ªÉn li√™n v√πng, t·ª´ ƒë√≥ n√¢ng cao hi·ªáu su·∫•t v·∫≠n h√†nh v√† gi·∫£m chi ph√≠ logistics tr√™n to√†n qu·ªëc.
T·ªânh Cao B·∫±ng Vi·ªát Nam: L√† ƒëi·ªÉm ƒë·∫ßu c·ªßa tuy·∫øn cao t·ªëc qu·ªëc gia, ƒë√≥ng vai tr√≤ ƒë·∫ßu m·ªëi k·∫øt n·ªëi v√πng ƒê√¥ng B·∫Øc. H·∫° t·∫ßng m·ªõi gi√∫p tƒÉng k·∫øt n·ªëi, t·∫°o c∆° h·ªôi thu h√∫t ƒë·∫ßu t∆∞ v√† ƒë·∫©y nhanh t·ªëc ƒë·ªô ph√°t tri·ªÉn kinh t·∫ø ƒë·ªãa ph∆∞∆°ng.
T·ªânh C√† Mau Vi·ªát Nam: L√† ƒëi·ªÉm cu·ªëi c·ªßa tuy·∫øn cao t·ªëc, v·ªõi h·ªá th·ªëng giao th√¥ng hi·ªán ƒë·∫°i gi√∫p m·ªü r·ªông th·ªã tr∆∞·ªùng du l·ªãch v√† ph√°t tri·ªÉn kinh t·∫ø v√πng ƒê·ªìng b·∫±ng s√¥ng C·ª≠u Long. T·∫°o l·ª£i th·∫ø c·∫°nh tranh m·ªõi cho ƒë·ªãa ph∆∞∆°ng.

[[NEGATIVE]]
B·ªô X√¢y d·ª±ng Vi·ªát Nam: R·ªßi ro ch·∫≠m ti·∫øn ƒë·ªô v√† ƒë·ªôi v·ªën n·∫øu ƒëi·ªÅu ph·ªëi kh√¥ng hi·ªáu qu·∫£ do s·ªë l∆∞·ª£ng d·ª± √°n tƒÉng g·∫•p nhi·ªÅu l·∫ßn.
Ch√≠nh quy·ªÅn ƒë·ªãa ph∆∞∆°ng Vi·ªát Nam: C√≥ th·ªÉ g·∫∑p kh√≥ khƒÉn trong gi·∫£i ph√≥ng m·∫∑t b·∫±ng v√† qu·∫£n l√Ω v·ªën ƒë·∫ßu t∆∞ n·∫øu nƒÉng l·ª±c t·ªï ch·ª©c y·∫øu.
Doanh nghi·ªáp x√¢y d·ª±ng Vi·ªát Nam: Thi c√¥ng ƒë·ªìng lo·∫°t nhi·ªÅu d·ª± √°n c√≥ th·ªÉ l√†m gi√£n m·ªèng nƒÉng l·ª±c nh√¢n s·ª± v√† m√°y m√≥c tƒÉng r·ªßi ro ch·∫≠m ti·∫øn ƒë·ªô ho·∫∑c gi·∫£m ch·∫•t l∆∞·ª£ng.
Doanh nghi·ªáp ngo√†i ng√†nh x√¢y d·ª±ng Vi·ªát Nam: Ch·ªãu t√°c ƒë·ªông gi√°n ti·∫øp t·ª´ chi ph√≠ logistics tƒÉng t·∫°m th·ªùi ho·∫∑c thi·∫øu h·ª•t nguy√™n v·∫≠t li·ªáu.

(K·∫æT TH√öC V√ç D·ª§)

Ng√†y ƒëƒÉng: {date}
Lo·∫°i ch·ªß ƒë·ªÅ: {group}
T·ª±a ƒë·ªÅ: {title}

M√¥ t·∫£: {description}


Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng:
""")

relation_extraction_template = PromptTemplate.from_template("""B·∫°n ƒëang l√†m vi·ªác d∆∞·ªõi b·ªëi c·∫£nh ph√¢n t√≠ch kinh t·∫ø.                                                            
H·∫°n ch·∫ø t·∫°o m·ªõi m·ªôt th·ª±c th·ªÉ, ch·ªâ ƒë∆∞·ª£c t·∫°o m·ªõi t·ªëi ƒëa 2 th·ª±c th·ªÉ m·ªõi. Ch·ªâ ƒë∆∞·ª£c li√™n k·∫øt t·ªõi 4 th·ª±c th·ªÉ kh√°c. Lu√¥n ∆∞u ti√™n li√™n k·∫øt v·ªõi c√°c th·ª±c th·ªÉ ƒë√£ c√≥: {existing_entities}

D·ª±a tr√™n t√°c ƒë·ªông ƒë·∫øn m·ªôt th·ª±c th·ªÉ, h√£y li·ªát k√™ c√°c th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng ti√™u c·ª±c v√† ·∫£nh h∆∞·ªüng t√≠ch c·ª±c do hi·ªáu ·ª©ng d√¢y chuy·ªÅn.
H√£y suy lu·∫≠n xem th·ª±c th·ªÉ hi·ªán t·∫°i n√†y c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ti·∫øp ƒë·∫øn nh·ªØng th·ª±c th·ªÉ kh√°c n√†o, theo h∆∞·ªõng t√≠ch c·ª±c ho·∫∑c ti√™u c·ª±c.
                                                            
V·ªõi m·ªói th·ª±c th·ªÉ, ·ªü ph·∫ßn T√™n th·ª±c th·ªÉ, h·∫°n ch·∫ø d√πng d·∫•u ch·∫•m, g·∫°ch ngang, d·∫•u v√† &, d·∫•u ch·∫•m ph·∫©y ;. C·∫ßn ghi th√™m qu·ªëc gia, ƒë·ªãa ph∆∞∆°ng c·ª• th·ªÉ v√† ng√†nh ngh·ªÅ c·ªßa n√≥ (n·∫øu c√≥). 
T√™n ch·ªâ n√≥i t·ªõi m·ªôt th·ª±c th·ªÉ duy nh·∫•t. Ph·∫ßn T√™n kh√¥ng ƒë∆∞·ª£c qu√° ph·ª©c t·∫°p, ƒë∆°n gi·∫£n nh·∫•t c√≥ th·ªÉ.
N·∫øu th·ª±c th·ªÉ n√†o thu·ªôc danh m·ª•c c·ªï phi·∫øu sau: {portfolio}, h√£y ghi r√µ t√™n c·ªï phi·∫øu.
V√≠ d·ª•: SSI-Ch·ª©ng kho√°n; Ng√†nh c√¥ng nghi·ªáp Vi·ªát Nam; Ng∆∞·ªùi d√πng M·ªπ; Ng√†nh th√©p Ch√¢u √Å; Ng√†nh du l·ªãch H·∫° Long, ...

Ghi nh·ªõ, H·∫°n ch·∫ø t·∫°o m·ªõi th·ª±c th·ªÉ, ch·ªâ ƒë∆∞·ª£c t·∫°o m·ªõi t·ªëi ƒëa 2 th·ª±c th·ªÉ m·ªõi. Ch·ªâ ƒë∆∞·ª£c li√™n k·∫øt t·ªõi 4 th·ª±c th·ªÉ kh√°c. Lu√¥n c·ªë li√™n k·∫øt v·ªõi c√°c th·ª±c th·ªÉ ƒë√£ c√≥.

Ph·∫ßn gi·∫£i th√≠ch m·ªói th·ª±c th·ªÉ, b·∫Øt bu·ªôc ƒë√°nh gi√° s·ªë li·ªáu ƒë∆∞·ª£c ghi, nhi·ªÅu ho·∫∑c √≠t, tƒÉng ho·∫∑c gi·∫£m, g·∫•p bao nhi√™u l·∫ßn, ...
C·∫ßn c·ªë g·∫Øng li√™n k·∫øt v·ªõi nhi·ªÅu th·ª±c th·ªÉ kh√°c. Tuy nhi√™n kh√¥ng suy ngo√†i ph·∫°m vi b√†i b√°o. Kh√¥ng t·ª± ch√®n s·ªë li·ªáu ngo√†i b√†i b√°o.
Kh√¥ng d√πng d·∫•u hai ch·∫•m trong ph·∫ßn gi·∫£i th√≠ch, ch·ªâ d√πng hai ch·∫•m : ƒë·ªÉ t√°ch gi·ªØa T√™n th·ª±c th·ªÉ v√† ph·∫ßn gi·∫£i th√≠ch.

ƒê∆∞a ra theo ƒë·ªãnh d·∫°ng sau:
[[POSITIVE]]
[Entity 1]: [Explanation]
...
[Entity N]: [Explanation]

[[NEGATIVE]]
[Entity A]: [Explanation]
..
[Entity Z]: [Explanation]

(B·∫ÆT ƒê·∫¶U V√ç D·ª§)

Th·ª±c th·ªÉ g·ªëc: B·ªô X√¢y d·ª±ng Vi·ªát Nam

·∫¢nh h∆∞·ªüng: √Åp l·ª±c qu·∫£n l√Ω 28 d·ª± √°n v·ªõi t·ªïng chi·ªÅu d√†i 1188 km, nh·∫±m hi·ªán th·ª±c h√≥a m·ª•c ti√™u ƒë·∫°t 3000 km cao t·ªëc v√†o nƒÉm 2025. S·ªë l∆∞·ª£ng d·ª± √°n tƒÉng g·∫•p nhi·ªÅu l·∫ßn so v·ªõi giai ƒëo·∫°n tr∆∞·ªõc, ƒë√≤i h·ªèi ƒëi·ªÅu ph·ªëi ngu·ªìn l·ª±c v√† ki·ªÉm so√°t ti·∫øn ƒë·ªô ch·∫∑t ch·∫Ω h∆°n.

Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng b·ªüi hi·ªáu ·ª©ng d√¢y chuy·ªÅn:

[[POSITIVE]]
Doanh nghi·ªáp x√¢y d·ª±ng Vi·ªát Nam: C√≥ c∆° h·ªôi m·ªü r·ªông h·ª£p ƒë·ªìng thi c√¥ng, tƒÉng doanh thu nh·ªù s·ªë l∆∞·ª£ng d·ª± √°n cao t·ªëc l·ªõn ƒëang tri·ªÉn khai ƒë·ªìng lo·∫°t.
Ng∆∞·ªùi lao ƒë·ªông Vi·ªát Nam: C√≥ th√™m nhi·ªÅu c∆° h·ªôi vi·ªác l√†m t·ª´ c√°c d·ª± √°n thi c√¥ng tr·∫£i d√†i kh·∫Øp c·∫£ n∆∞·ªõc.

[[NEGATIVE]]
B·ªô Giao th√¥ng V·∫≠n t·∫£i Vi·ªát Nam: Ch·ªãu √°p l·ª±c ph·ªëi h·ª£p v√† gi√°m s√°t hi·ªáu qu·∫£ gi·ªØa c√°c b√™n li√™n quan, c√≥ nguy c∆° b·ªã ch·ªâ tr√≠ch n·∫øu d·ª± √°n ch·∫≠m ti·∫øn ƒë·ªô.
Doanh nghi·ªáp x√¢y d·ª±ng Vi·ªát Nam: C√≥ th·ªÉ ch·ªãu √°p l·ª±c tƒÉng gi√° nguy√™n v·∫≠t li·ªáu v√† thi·∫øu h·ª•t ngu·ªìn cung do nhu c·∫ßu tƒÉng ƒë·ªôt bi·∫øn.

(K·∫æT TH√öC V√ç D·ª§)

Th·ª±c th·ªÉ g·ªëc: {entities}

·∫¢nh h∆∞·ªüng: {description}

Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng b·ªüi hi·ªáu ·ª©ng d√¢y chuy·ªÅn:
""")

# reasoning_template = PromptTemplate.from_template("""
# Cho danh m·ª•c c·ªï phi·∫øu sau:
# {portfolio}

# Cho m·ªôt ƒë·ªì th·ªã tri th·ª©c ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng quan h·ªá th·ªùi gian v√† t√°c ƒë·ªông, ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng c√°c tuple 
# (th·ªùi gian, ngu·ªìn, h√†nh ƒë·ªông, ƒë√≠ch), c·∫ßn d·ª± ƒëo√°n li·ªáu danh m·ª•c c·ªï phi·∫øu ƒë√£ n√™u c√≥ "s·∫≠p gi√°" trong ng√†y ti·∫øp theo hay kh√¥ng.
                                                  
# D∆∞·ªõi ƒë√¢y l√† ƒë·ªì th·ªã tri th·ª©c ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng c√°c tuple (th·ªùi gian, ngu·ªìn, h√†nh ƒë·ªông, ƒë√≠ch):
# {tuples}

# L∆∞u √Ω r·∫±ng "s·∫≠p gi√°" ·ªü ƒë√¢y bi·ªÉu th·ªã cho m·ªôt ƒë·ª£t gi·∫£m gi√° c·ªßa c·ªï phi·∫øu R·∫§T M·∫†NH (l√™n t·ªõi 5%). V√¨ v·∫≠y c·∫ßn c√≥ ph√¢n t√≠ch ƒëa chi·ªÅu, t·ª´ nhi·ªÅu ph√≠a kh√°c nhau.
# Gi·∫£i th√≠ch theo t·ª´ng b∆∞·ªõc cho l·ª±a ch·ªçn ƒë√≥.

# S·ª≠ d·ª•ng l√Ω lu·∫≠n c·ªßa ri√™ng b·∫°n tr√™n ƒë·ªì th·ªã ƒë∆∞·ª£c ƒë∆∞a, v√† kh√¥ng ƒë·ªÅ c·∫≠p ƒë·∫øn c√°c s·ª± ki·ªán kh√°c ngo√†i ƒë·ªì th·ªã c√≥ trong qu√° kh·ª©.

# D·ª± ƒëo√°n d∆∞·ªõi ƒë·ªãnh d·∫°ng sau:
# Explanation: [L√Ω do]
# Crash: [Yes/No]

# L√Ω do s·∫≠p gi√° cho chu·ªói s·ª± ki·ªán tr√™n: """)
# S·ªë b√†i b√°o c·ªë ƒë·ªãnh cho m·ªói ng√†y d·ª± ƒëo√°n
ARTICLES_PER_DATE = 20

# S·ª≠a reasoning_template ƒë·ªÉ th√™m prediction_date
reasoning_template = PromptTemplate.from_template("""
D·ª± ƒëo√°n li·ªáu danh m·ª•c c·ªï phi·∫øu sau c√≥ s·∫≠p gi√° v√†o ng√†y {prediction_date} hay kh√¥ng:

Danh m·ª•c c·ªï phi·∫øu:
{portfolio}

Cho m·ªôt ƒë·ªì th·ªã tri th·ª©c ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng quan h·ªá th·ªùi gian v√† t√°c ƒë·ªông, ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng c√°c tuple 
(th·ªùi gian, ngu·ªìn, h√†nh ƒë·ªông, ƒë√≠ch), c·∫ßn d·ª± ƒëo√°n li·ªáu danh m·ª•c c·ªï phi·∫øu ƒë√£ n√™u c√≥ s·∫≠p gi√° v√†o ng√†y {prediction_date} hay kh√¥ng.
                                                  
D∆∞·ªõi ƒë√¢y l√† ƒë·ªì th·ªã tri th·ª©c ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng c√°c tuple (th·ªùi gian, ngu·ªìn, h√†nh ƒë·ªông, ƒë√≠ch):
{tuples}

L∆∞u √Ω r·∫±ng "s·∫≠p gi√°" ·ªü ƒë√¢y bi·ªÉu th·ªã cho m·ªôt ƒë·ª£t gi·∫£m gi√° c·ªßa c·ªï phi·∫øu R·∫§T M·∫†NH (l√™n t·ªõi 5%). V√¨ v·∫≠y c·∫ßn c√≥ ph√¢n t√≠ch ƒëa chi·ªÅu, t·ª´ nhi·ªÅu ph√≠a kh√°c nhau.
Gi·∫£i th√≠ch theo t·ª´ng b∆∞·ªõc cho l·ª±a ch·ªçn ƒë√≥, v√† n√™u r√µ r·∫±ng d·ª± ƒëo√°n l√† cho ng√†y {prediction_date}.

S·ª≠ d·ª•ng l√Ω lu·∫≠n c·ªßa ri√™ng b·∫°n tr√™n ƒë·ªì th·ªã ƒë∆∞·ª£c ƒë∆∞a, v√† kh√¥ng ƒë·ªÅ c·∫≠p ƒë·∫øn c√°c s·ª± ki·ªán kh√°c ngo√†i ƒë·ªì th·ªã c√≥ trong qu√° kh·ª©.

D·ª± ƒëo√°n d∆∞·ªõi ƒë·ªãnh d·∫°ng sau:
Explanation: [L√Ω do]
Crash: [Yes/No]

L√Ω do s·∫≠p gi√° cho chu·ªói s·ª± ki·ªán v√†o ng√†y {prediction_date}: 
""")

batch_relation_extraction_template = PromptTemplate.from_template("""B·∫°n ƒëang l√†m vi·ªác d∆∞·ªõi b·ªëi c·∫£nh ph√¢n t√≠ch kinh t·∫ø.
H·∫°n ch·∫ø t·∫°o m·ªõi th·ª±c th·ªÉ, ch·ªâ ƒë∆∞·ª£c t·∫°o m·ªõi t·ªëi ƒëa 2 th·ª±c th·ªÉ m·ªõi cho m·ªói th·ª±c th·ªÉ g·ªëc. Ch·ªâ ƒë∆∞·ª£c li√™n k·∫øt t·ªëi ƒëa 3 th·ª±c th·ªÉ kh√°c cho m·ªói th·ª±c th·ªÉ g·ªëc. Lu√¥n ∆∞u ti√™n li√™n k·∫øt v·ªõi c√°c th·ª±c th·ªÉ ƒë√£ c√≥: {existing_entities}

D·ª±a tr√™n t√°c ƒë·ªông ƒë·∫øn c√°c th·ª±c th·ªÉ ƒë·∫ßu v√†o, h√£y ph√¢n t√≠ch hi·ªáu ·ª©ng d√¢y chuy·ªÅn. 
H√£y suy lu·∫≠n xem m·ªói th·ª±c th·ªÉ hi·ªán t·∫°i c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ti·∫øp ƒë·∫øn nh·ªØng th·ª±c th·ªÉ kh√°c n√†o, theo h∆∞·ªõng t√≠ch c·ª±c ho·∫∑c ti√™u c·ª±c.

V·ªõi m·ªói th·ª±c th·ªÉ, ·ªü ph·∫ßn T√™n th·ª±c th·ªÉ, h·∫°n ch·∫ø d√πng d·∫•u ch·∫•m, g·∫°ch ngang, d·∫•u v√† &, d·∫•u ch·∫•m ph·∫©y ;. C·∫ßn ghi th√™m qu·ªëc gia, ƒë·ªãa ph∆∞∆°ng c·ª• th·ªÉ v√† ng√†nh ngh·ªÅ c·ªßa n√≥ (n·∫øu c√≥).
T√™n ch·ªâ n√≥i t·ªõi m·ªôt th·ª±c th·ªÉ duy nh·∫•t. Ph·∫ßn T√™n kh√¥ng ƒë∆∞·ª£c qu√° ph·ª©c t·∫°p, ƒë∆°n gi·∫£n nh·∫•t c√≥ th·ªÉ.
N·∫øu th·ª±c th·ªÉ n√†o thu·ªôc danh m·ª•c c·ªï phi·∫øu sau: {portfolio}, h√£y ghi r√µ t√™n c·ªï phi·∫øu.
V√≠ d·ª•: SSI-Ch·ª©ng kho√°n; Ng√†nh c√¥ng nghi·ªáp Vi·ªát Nam; Ng∆∞·ªùi d√πng M·ªπ; Ng√†nh th√©p Ch√¢u √Å; Ng√†nh du l·ªãch H·∫° Long, ...

Ph·∫ßn gi·∫£i th√≠ch m·ªói th·ª±c th·ªÉ, b·∫Øt bu·ªôc ƒë√°nh gi√° s·ªë li·ªáu ƒë∆∞·ª£c ghi, nhi·ªÅu ho·∫∑c √≠t, tƒÉng ho·∫∑c gi·∫£m, g·∫•p bao nhi√™u l·∫ßn...
C·∫ßn c·ªë g·∫Øng li√™n k·∫øt v·ªõi nhi·ªÅu th·ª±c th·ªÉ kh√°c. Tuy nhi√™n kh√¥ng suy ngo√†i ph·∫°m vi b√†i b√°o. Kh√¥ng t·ª± ch√®n s·ªë li·ªáu ngo√†i b√†i b√°o.
Kh√¥ng d√πng d·∫•u hai ch·∫•m trong ph·∫ßn gi·∫£i th√≠ch, ch·ªâ d√πng hai ch·∫•m : ƒë·ªÉ t√°ch gi·ªØa T√™n th·ª±c th·ªÉ v√† ph·∫ßn gi·∫£i th√≠ch.

ƒê∆∞a ra theo ƒë·ªãnh d·∫°ng sau cho m·ªói th·ª±c th·ªÉ ngu·ªìn:

[[SOURCE: T√™n th·ª±c th·ªÉ ngu·ªìn]]
[[IMPACT: POSITIVE/NEGATIVE]]

[[POSITIVE]]
[Th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng 1]: [Gi·∫£i th√≠ch]
[Th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng 2]: [Gi·∫£i th√≠ch]
[Th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng 3]: [Gi·∫£i th√≠ch]

[[NEGATIVE]]
[Th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng A]: [Gi·∫£i th√≠ch]
[Th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng B]: [Gi·∫£i th√≠ch]
[Th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng C]: [Gi·∫£i th√≠ch]

B·∫°n s·∫Ω ph√¢n t√≠ch nhi·ªÅu th·ª±c th·ªÉ g·ªëc m·ªôt l√∫c. V·ªõi T·ª™NG th·ª±c th·ªÉ, ch·ªâ ch·ªçn CH√çNH X√ÅC 2-3 th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng t√≠ch c·ª±c v√† 2-3 th·ª±c th·ªÉ ·∫£nh h∆∞·ªüng ti√™u c·ª±c quan tr·ªçng nh·∫•t.

L∆ØU √ù: C√≥ th·ªÉ c√≥ R·∫§T NHI·ªÄU th·ª±c th·ªÉ ƒë·∫ßu v√†o, h√£y ph√¢n t√≠ch C·∫®N TH·∫¨N t·ª´ng th·ª±c th·ªÉ ƒë·ªÉ kh√¥ng b·ªè s√≥t. Kh√¥ng ƒë∆∞·ª£c t·∫°o th√™m th·ª±c th·ªÉ g·ªëc.
                                                                  
(B·∫ÆT ƒê·∫¶U V√ç D·ª§)
Danh s√°ch th·ª±c th·ªÉ ngu·ªìn:

Th·ª±c th·ªÉ g·ªëc: B·ªô X√¢y d·ª±ng Vi·ªát Nam

·∫¢nh h∆∞·ªüng: NEGATIVE, √Åp l·ª±c qu·∫£n l√Ω 28 d·ª± √°n v·ªõi t·ªïng chi·ªÅu d√†i 1188 km, nh·∫±m hi·ªán th·ª±c h√≥a m·ª•c ti√™u ƒë·∫°t 3000 km cao t·ªëc v√†o nƒÉm 2025. S·ªë l∆∞·ª£ng d·ª± √°n tƒÉng g·∫•p nhi·ªÅu l·∫ßn so v·ªõi giai ƒëo·∫°n tr∆∞·ªõc, ƒë√≤i h·ªèi ƒëi·ªÅu ph·ªëi ngu·ªìn l·ª±c v√† ki·ªÉm so√°t ti·∫øn ƒë·ªô ch·∫∑t ch·∫Ω h∆°n.

---

Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng b·ªüi hi·ªáu ·ª©ng d√¢y chuy·ªÅn:

[[SOURCE: B·ªô X√¢y d·ª±ng Vi·ªát Nam]]
[[IMPACT: NEGATIVE]]

[[POSITIVE]]
Doanh nghi·ªáp x√¢y d·ª±ng Vi·ªát Nam: C√≥ c∆° h·ªôi m·ªü r·ªông h·ª£p ƒë·ªìng thi c√¥ng, tƒÉng doanh thu nh·ªù s·ªë l∆∞·ª£ng d·ª± √°n cao t·ªëc l·ªõn ƒëang tri·ªÉn khai ƒë·ªìng lo·∫°t.
Ng∆∞·ªùi lao ƒë·ªông Vi·ªát Nam: C√≥ th√™m nhi·ªÅu c∆° h·ªôi vi·ªác l√†m t·ª´ c√°c d·ª± √°n thi c√¥ng tr·∫£i d√†i kh·∫Øp c·∫£ n∆∞·ªõc.

[[NEGATIVE]]
B·ªô Giao th√¥ng V·∫≠n t·∫£i Vi·ªát Nam: Ch·ªãu √°p l·ª±c ph·ªëi h·ª£p v√† gi√°m s√°t hi·ªáu qu·∫£ gi·ªØa c√°c b√™n li√™n quan, c√≥ nguy c∆° b·ªã ch·ªâ tr√≠ch n·∫øu d·ª± √°n ch·∫≠m ti·∫øn ƒë·ªô.
Doanh nghi·ªáp x√¢y d·ª±ng Vi·ªát Nam: C√≥ th·ªÉ ch·ªãu √°p l·ª±c tƒÉng gi√° nguy√™n v·∫≠t li·ªáu v√† thi·∫øu h·ª•t ngu·ªìn cung do nhu c·∫ßu tƒÉng ƒë·ªôt bi·∫øn.

(K·∫æT TH√öC V√ç D·ª§)

Danh s√°ch th·ª±c th·ªÉ ngu·ªìn:

{input_entities}

Danh s√°ch th·ª±c th·ªÉ s·∫Ω b·ªã ·∫£nh h∆∞·ªüng b·ªüi hi·ªáu ·ª©ng d√¢y chuy·ªÅn:
""")

# Create separate chains for each prompt
chain_summary = news_summarize_template | model
time.sleep(1)  # Delay sau khi kh·ªüi t·∫°o chain
chain_summary_more_temperature = news_summarize_template | model_more_temperature
time.sleep(1)
chain_summary_pro = news_summarize_template | model2
time.sleep(1)
chain_entity = entity_extraction_template | model
time.sleep(1)
chain_relation = relation_extraction_template | model
time.sleep(1)
chain_reasoning = reasoning_template | model_more_temperature
time.sleep(1)

# Create chain for batch processing
chain_batch_relation = batch_relation_extraction_template | model
time.sleep(1)


# ---------------------------
# Helper Functions
# ---------------------------
def read_news_data(csv_path="cleaned_posts.csv"):
    """
    Reads the CSV file and converts the ISO date strings using dateutil.
    """
    df = pd.read_csv(csv_path)
    df['parsed_date'] = df['date'].apply(isoparse)
    return df

def build_article_text(row):
    """
    Build the text for an article based on its columns.
    """
    return (f"Ng√†y ƒëƒÉng: {row['date']}\n"
            f"Lo·∫°i ch·ªß ƒë·ªÅ: {row['group']}\n"
            f"T·ª±a ƒë·ªÅ: {row['title']}\n\n"
            f"M√¥ t·∫£: {row['description']}")

def combine_articles(df: pd.DataFrame) -> str:
    """
    K·∫øt h·ª£p c√°c b√†i b√°o trong ng√†y th√†nh m·ªôt chu·ªói vƒÉn b·∫£n.
    """
    articles = [f"Ch·ªß ƒë·ªÅ {row['group']}:\n{row['title']}, {row['description']}\n" 
                for idx, row in df.iterrows()]
    return "\n".join(articles)

def parse_summary_response(response, date, starting_index=1):
    """
    Ph√¢n t√≠ch ƒë·∫ßu ra c·ªßa LLM theo ƒë·ªãnh d·∫°ng.
    """
    pattern = r'^(.+?):\s*(.+?)\s*\|\s*(.+)$'
    response_text = str(response.content)
    lines = response_text.splitlines()
    articles = []

    for line in lines:
        line = line.strip()
        if not line:
            continue
        match = re.match(pattern, line)
        if match:
            group, title, description = match.groups()
            articles.append({
                "postID": starting_index,
                "title": title.strip('[').strip(']').strip(),
                "description": description.strip('[').strip(']').strip(),
                "date": date,
                "group": group.strip('[').strip(']').strip()
            })
            starting_index += 1
    
    return pd.DataFrame(articles)

def parse_entity_response(response):
    """
    Parses the response from the entity extraction prompt.
    """
    if response is None:
        print("Response is None")
        return {"POSITIVE": [], "NEGATIVE": []}
        
    sections = {"POSITIVE": [], "NEGATIVE": []}
    current_section = None
    str_resp = response.content
    
    for line in str(str_resp).splitlines():
        line = line.strip()
        if not line:
            continue
        if "[[POSITIVE]]" in line.upper():
            current_section = "POSITIVE"
            continue
        if "[[NEGATIVE]]" in line.upper():
            current_section = "NEGATIVE"
            continue
        if current_section and ':' in line:
            entity = line.split(":", 1)[0].strip()
            # Skip invalid entities
            if not entity or "kh√¥ng c√≥ th·ª±c th·ªÉ n√†o" in entity.lower():
                continue
            # content = all line except entity
            content = line.split(entity, 1)[-1].strip(':').strip()
            sections[current_section].append((entity, content))

    return sections

def merge_entity(entity, canonical_set):
    """
    Returns the canonical version of the entity if already present (case-insensitive),
    otherwise adds and returns the new entity.
    """
    normalized_entity = str(entity).strip('[').strip(']').strip(' ').lower()
    for exist in canonical_set:
        if exist.lower() == normalized_entity:
            return exist
    canonical_set.add(normalized_entity)
    return normalized_entity

def add_edge(G, source, target, impact, timestamp):
    """
    Adds an edge to the graph if it does not already exist.
    """
    if not G.has_edge(source, target):
        G.add_edge(source, target, impact=impact, timestamp=timestamp)

def graph_to_tuples(G):
    """
    Converts the graph to a string of tuples (date, source, impact, target).
    """
    tuples = []
    for u, v, data in G.edges(data=True):
        # Extract the date as a string, handling different timestamp formats
        timestamp = data.get("timestamp")
        if timestamp is None:
            # Skip edges without timestamps
            continue
            
        try:
            # Handle different timestamp formats
            if isinstance(timestamp, pd.Timestamp):
                date_str = timestamp.date().isoformat()
            elif hasattr(timestamp, "date"):  # datetime object
                date_str = timestamp.date().isoformat()
            elif isinstance(timestamp, (int, float)):  # Unix timestamp
                date_str = pd.Timestamp(timestamp, unit='s').date().isoformat()
            else:  # Try to parse as string
                parsed_date = pd.to_datetime(timestamp)
                date_str = parsed_date.date().isoformat()
                
            # Skip invalid entities in output tuples
            if "kh√¥ng c√≥ th·ª±c th·ªÉ n√†o" in str(u).lower() or "kh√¥ng c√≥ th·ª±c th·ªÉ n√†o" in str(v).lower():
                continue
                
            tuples.append(f"({date_str}, {u}, {data.get('impact')} TO, {v})")
        except Exception as e:
            print(f"Error processing edge ({u}, {v}): {e}, timestamp: {timestamp}, type: {type(timestamp)}")
            continue

    # Sort tuples by ascending order of date

    return "\n".join(sorted(tuples))

def update_edge_decay_weights(G: nx.DiGraph, current_time=None, lambda_decay=1) -> nx.DiGraph:
    """
    Updates each edge's weight based on exponential decay from its timestamp.
    """
    if current_time is None:
        print("\n\nCurrent time is None, using current timestamp\n")
        current_time = pd.Timestamp.now()
    
    decay_weights = dict()
    for u, v, data in G.edges(data=True):
        u_timestamp = G.nodes[u].get("timestamp")
        v_timestamp = G.nodes[v].get("timestamp")
        # test print
        delta = (int)((v_timestamp - u_timestamp).total_seconds()/86400)
        R_decay = math.exp(-delta / (lambda_decay))
        data["weight"] = 0.0 if delta < 0 else R_decay
        decay_weights[delta] = data["weight"]
    
    # Print decay weights after sort by key
    print(f"Decay weights: {sorted(decay_weights.items())}")
    return G

# def attention_phase(G, current_time, lambda_decay, q=6):
#     """
#     Uses TPPR (Temporal Personalized PageRank) to find important entities and their connections.
#     Filters out future edges and applies time-decayed weights.
#     """

#     import pandas as pd
#     import networkx as nx
#     import math

#     # üîß First, create a copy of the graph to avoid modifying the original
#     G_filtered = G.copy()

#     # üîß Filter out future edges
#     edges_to_remove = []
#     for u, v, data in G_filtered.edges(data=True):
#         edge_time = data.get("timestamp")

#         if edge_time is None:
#             continue

#         try:
#             if isinstance(edge_time, pd.Timestamp) or hasattr(edge_time, "timestamp"):
#                 edge_timestamp = edge_time.timestamp()
#             elif isinstance(edge_time, (int, float)):
#                 edge_timestamp = edge_time
#             else:
#                 edge_timestamp = pd.Timestamp(edge_time).timestamp()

#             current_timestamp = current_time if isinstance(current_time, (int, float)) else pd.Timestamp(current_time).timestamp()
#             if edge_timestamp > current_timestamp:
#                 edges_to_remove.append((u, v))
#         except Exception as e:
#             print(f"Warning: Could not process timestamp {edge_time} for edge ({u}, {v}): {e}")
#             continue

#     for u, v in edges_to_remove:
#         G_filtered.remove_edge(u, v)

#     print(f"Filtered out {len(edges_to_remove)} future edges from graph for TPPR calculation")

#     # üîß Apply time decay to remaining edges
#     def update_edge_decay_weights(G, current_time, lambda_decay):
#         for u, v, data in G.edges(data=True):
#             ts = data.get("timestamp")
#             if ts is None:
#                 continue
#             try:
#                 ts_val = ts if isinstance(ts, (int, float)) else pd.Timestamp(ts).timestamp()
#                 cur_time_val = current_time if isinstance(current_time, (int, float)) else pd.Timestamp(current_time).timestamp()
#                 decay = math.exp(-lambda_decay * (cur_time_val - ts_val))
#                 G[u][v]['weight'] = G[u][v].get('weight', 1.0) * decay
#             except:
#                 continue
#         return G

#     G_filtered = update_edge_decay_weights(G_filtered, current_time, lambda_decay)  # üîß

#     # üîß Temporal Personalized PageRank (TPPR)
#     def temporal_pagerank(G, alpha=0.85, max_iter=100, tol=1.0e-6):
#         nodes = list(G.nodes())
#         N = len(nodes)
#         rank = dict.fromkeys(nodes, 1.0 / N)
#         for _ in range(max_iter):
#             prev_rank = rank.copy()
#             for n in nodes:
#                 rank[n] = (1.0 - alpha) / N
#             for u, v in G.edges():
#                 if G.out_degree(u) > 0:
#                     rank[v] += alpha * (prev_rank[u] * G[u][v].get("weight", 1.0)) / G.out_degree(u)
#             diff = sum(abs(rank[n] - prev_rank[n]) for n in nodes)
#             if diff < tol:
#                 break
#         return rank

#     pr_scores = temporal_pagerank(G_filtered)  # üîß

#     # Filter to entity or stock nodes
#     filtered_scores = {node: score for node, score in pr_scores.items()
#                        if G_filtered.nodes[node].get("type") in ["entity", "stock"]}

#     top_nodes = sorted(filtered_scores, key=filtered_scores.get, reverse=True)[:q]

#     for node, score in sorted(filtered_scores.items(), key=lambda x: x[1], reverse=True)[:q]:
#         print(f"{node}: {score}")

#     selected_nodes = set(top_nodes)
#     for node in top_nodes:
#         if node in G_filtered:
#             selected_nodes.update(G_filtered.predecessors(node))
#             selected_nodes.update(G_filtered.successors(node))

#     sub_G = G_filtered.subgraph(selected_nodes).copy()

#     print(f"Created subgraph with {sub_G.number_of_nodes()} nodes and {sub_G.number_of_edges()} edges")
#     print(f"Top 10 nodes by number of incoming edges: {sorted(sub_G.in_degree(), key=lambda x: x[1], reverse=True)[:10]}")

#     return sub_G
def apply_tppr_decay_weights(G, current_time, lambda_decay):
    """
    Applies time decay to edge weights based on the TPPR formula.
    
    Parameters:
    -----------
    G : nx.DiGraph
        The input graph (G_temporal)
    current_time : float or datetime
        The current timestamp for decay calculation
    lambda_decay : float
        Decay factor for edge weights
    
    Returns:
    --------
    nx.DiGraph
        Graph with updated edge weights
    """
    for u, v, data in G.edges(data=True):
        ts = data.get("timestamp")
        if ts is None:
            continue
        try:
            ts_val = ts if isinstance(ts, (int, float)) else pd.Timestamp(ts).timestamp()
            cur_time_val = current_time if isinstance(current_time, (int, float)) else pd.Timestamp(current_time).timestamp()
            decay = math.exp(-lambda_decay * (cur_time_val - ts_val))
            G[u][v]['weight'] = G[u][v].get('weight', 1.0) * decay
        except Exception as e:
            print(f"Warning: Could not apply decay for edge ({u}, {v}): {e}")
            continue
    return G
def attention_phase(G, current_time, lambda_decay, q=6):
    """
    Uses Temporal Personalized PageRank (TPPR) to find important entities and their connections.
    Creates a filtered copy that only uses edges dated before or on the prediction date.
    Applies time-decayed weights and personalizes ranking based on portfolio stocks and sectors.
    
    Parameters:
    - G: NetworkX DiGraph, the temporal-relational graph (G_temporal)
    - current_time: Timestamp or datetime, the current time for filtering future edges
    - lambda_decay: Float, decay rate for memory retention (default=1.0 as per paper)
    - q: Int, number of top entities to select (default=6 as per paper)
    
    Returns:
    - sub_G: NetworkX DiGraph, the subgraph (G_TRR) containing top-q entities and their neighbors
    """
    #  Create a copy of the graph to avoid modifying the original
    G_filtered = G.copy()
    
    #  Filter out future edges
    edges_to_remove = []
    for u, v, data in G_filtered.edges(data=True):
        edge_time = data.get("timestamp")
        
        # Skip edges without timestamps
        if edge_time is None:
            continue
            
        # Convert to timestamp for comparison
        try:
            if isinstance(edge_time, pd.Timestamp) or hasattr(edge_time, "timestamp"):
                edge_timestamp = edge_time.timestamp()
            elif isinstance(edge_time, (int, float)):
                edge_timestamp = edge_time
            else:
                edge_timestamp = pd.Timestamp(edge_time).timestamp()
            
            current_timestamp = current_time if isinstance(current_time, (int, float)) else pd.Timestamp(current_time).timestamp()
            if edge_timestamp > current_timestamp:
                edges_to_remove.append((u, v))
        except Exception as e:
            print(f"Warning: Could not process timestamp {edge_time} for edge ({u}, {v}): {e}")
            continue
            
    # Remove future edges
    for u, v in edges_to_remove:
        G_filtered.remove_edge(u, v)
        
    print(f"Filtered out {len(edges_to_remove)} future edges from graph for TPPR calculation")
    
    # Step 1: Apply update_edge_decay_weights to create G_temporal
    print("Creating G_temporal by applying temporal decay weights...")
    G_temporal = update_edge_decay_weights(G_filtered, current_time=current_time, lambda_decay=lambda_decay)
    
    # Step 2: Apply TPPR decay weights to G_temporal
    print("Applying TPPR decay weights to G_temporal...")
    G_temporal = apply_tppr_decay_weights(G_temporal, current_time, lambda_decay)

    #  Create personalization vector for TPPR
    personalization = {}
    total_nodes = len(G_filtered.nodes())
    for node in G_filtered.nodes():
        node_type = G_filtered.nodes[node].get("type")
        node_sector = G_filtered.nodes[node].get("sector", "")
        
        # Prioritize portfolio stocks
        if node in PORTFOLIO_STOCKS:
            personalization[node] = 0.1  # High priority for portfolio stocks
        # Prioritize entities in portfolio sectors
        elif node_sector in PORTFOLIO_SECTOR:
            personalization[node] = 0.05  # Medium priority for related sectors
        # Default for other nodes
        else:
            personalization[node] = 1.0 / total_nodes  # Low priority for others
    
    #  Compute Temporal Personalized PageRank (TPPR) scores
    pr_scores = nx.pagerank(G_filtered, alpha=0.85, personalization=personalization, weight="weight")
    
    # Filter to entity or stock nodes
    filtered_scores = {node: score for node, score in pr_scores.items()
                      if G_filtered.nodes[node].get("type") in ["entity", "stock"]}
    
    #  Get top q nodes
    top_nodes = sorted(filtered_scores, key=filtered_scores.get, reverse=True)[:q]
    
    # Print top nodes and their scores
    for node, score in sorted(filtered_scores.items(), key=lambda x: x[1], reverse=True)[:q]:
        print(f"{node}: {score}")
    
    #  Include their immediate neighbors
    selected_nodes = set(top_nodes)
    for node in top_nodes:
        if node in G_filtered:
            selected_nodes.update(G_filtered.predecessors(node))
            selected_nodes.update(G_filtered.successors(node))
    
    #  Create the subgraph (G_TRR)
    sub_G = G_filtered.subgraph(selected_nodes).copy()
    
    print(f"Created subgraph with {sub_G.number_of_nodes()} nodes and {sub_G.number_of_edges()} edges")
    print(f"Top 10 nodes by number of incoming edges: {sorted(sub_G.in_degree(), key=lambda x: x[1], reverse=True)[:10]}")
    
    return sub_G

def graph_entities_to_str(G):
    """
    Get string of "entity" type nodes from graph for context
    """
    entities = [node for node in G.nodes() if G.nodes[node].get("type") == "entity"]
    graph_str = ", ".join(entities)
    return graph_str[:20000] if len(graph_str) > 20000 else graph_str

def invoke_chain_with_retry(chain, prompt, max_retries=MAX_RETRIES, base_delay=BASE_DELAY):
    """
    Invokes a chain with exponential backoff retry logic
    """
    retry_count = 0
    while True:
        try:
            response = chain.invoke(prompt)
            return response
        except Exception as e:
            retry_count += 1
            if retry_count > max_retries:
                print(f"Maximum retries reached. Error: {e}")
                return None
                
            delay = base_delay * (2 ** (retry_count - 1))
            print(f"Error: {e}. Retrying in {delay} seconds... (Attempt {retry_count}/{max_retries})")
            time.sleep(delay)

def process_entity_relationships(entity_info, G: nx.DiGraph, canonical_entities, portfolio, portfolio_str_full, article_timestamp):
    """
    Process a single entity's relationships using the relation extraction chain
    Returns a list of new entities to process
    """
    entity, impact, content = entity_info
    next_entities = []
    
    # Create prompt for relation extraction
    prompt_rel = {
        "entities": entity,
        "portfolio": portfolio_str_full,
        "description": impact + ", " + content,
        "existing_entities": graph_entities_to_str(G)
    }
    
    # Get relationships with retry logic
    response_rel = invoke_chain_with_retry(chain_relation, prompt_rel)
    time.sleep(1)  # Rate limiting
    
    if response_rel is None:
        return []
        
    # Process the response
    rel_dict = parse_entity_response(response_rel)
    
    # Add edges and collect new entities
    for new_ent, content_new in rel_dict.get(impact, []):
        canon_new = merge_entity(new_ent, canonical_entities)
        node_type = "stock" if any(str(canon_new).lower().find(stock.lower()) != -1 for stock in portfolio) else "entity"
        
        # Add node if it doesn't exist
        if not G.has_node(canon_new):
            G.add_node(canon_new, type=node_type, timestamp=article_timestamp)
            
        # Add edge
        add_edge(G, entity, canon_new, impact, article_timestamp)
        
        # Add to frontier if it's an entity (not a stock)
        if node_type == "entity":
            next_entities.append((canon_new, impact, content_new))
            
    return next_entities

def parse_batch_entity_response(response):
    """
    Parses the response from the batch entity relation extraction prompt.
    Returns a list of tuples (source_entity, impact, target_entity, content)
    """
    if response is None:
        print("Response is None")
        return []
        
    results = []
    current_source = None
    current_impact = None
    current_section = None
    
    str_resp = str(response.content)
    lines = str_resp.splitlines()
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Check for source entity marker
        if line.startswith("[[SOURCE:") or "[[SOURCE:" in line:
            source_text = line.replace("[[SOURCE:", "").replace("]]", "").strip()
            if source_text and "kh√¥ng c√≥ th·ª±c th·ªÉ n√†o" not in source_text.lower():
                current_source = source_text
            else:
                current_source = None  # Skip invalid source entities
            continue
            
        # Check for impact marker
        if line.startswith("[[IMPACT:") or "[[IMPACT:" in line:
            impact_str = line.replace("[[IMPACT:", "").replace("]]", "").strip()
            current_impact = impact_str.upper()
            continue
            
        # Check for positive/negative section markers
        if "[[POSITIVE]]" in line.upper():
            current_section = "POSITIVE"
            continue
            
        if "[[NEGATIVE]]" in line.upper():
            current_section = "NEGATIVE"
            continue
            
        # Process entity and explanation if we're in a valid context
        if current_source and current_section and ':' in line:
            # Extract entity name and content
            try:
                entity, *content_parts = line.split(":", 1)
                entity = entity.strip().strip('[]')  # Remove any potential brackets
                
                # Skip invalid target entities
                if not entity or "kh√¥ng c√≥ th·ª±c th·ªÉ n√†o" in entity.lower():
                    continue
                    
                if entity and content_parts:
                    content = content_parts[0].strip()
                    # Use the current_impact if specified, otherwise use the current section
                    actual_impact = current_impact if current_impact else current_section
                    results.append((current_source, actual_impact, entity, content))
            except Exception as e:
                print(f"Error parsing line: {line}. Error: {e}")
                continue
    
    if not results:
        print("Warning: No relationships were parsed from the response")
        print(f"Response content: {str_resp[:500]}...")
        
    return results

def batch_process_entity_relationships(entity_batch, G, canonical_entities, portfolio, portfolio_str_full, article_timestamp):
    """
    Process multiple entities in a single API call
    Returns a list of new entities to process
    """
    if not entity_batch:
        return []
    
    # Maximum retries for batch processing    
    max_batch_retries = 2
    batch_retry_count = 0
    relationships = []
    
    while batch_retry_count < max_batch_retries:
        # Format the input entities for the prompt more explicitly
        input_entities_text = ""
        for entity, impact, content in entity_batch:
            # Format similar to the relation_extraction_template structure
            input_entities_text += f"Th·ª±c th·ªÉ g·ªëc: {entity}\n\n·∫¢nh h∆∞·ªüng: {impact}, {content}\n\n---\n\n"
        
        # Create prompt for batch relation extraction
        prompt_batch = {
            "input_entities": input_entities_text,
            "portfolio": portfolio_str_full,
            "existing_entities": graph_entities_to_str(G)
        }
        
        # Get relationships with retry logic
        response = invoke_chain_with_retry(chain_batch_relation, prompt_batch)
        time.sleep(1)  # Rate limiting
        
        if response is None:
            return []
        
        # Parse the response to get all relationships
        relationships = parse_batch_entity_response(response)
        
        # Check if we got any relationships
        if len(relationships) > 0:
            break
            
        batch_retry_count += 1
        print(f"Batch processing returned 0 relationships. Retry {batch_retry_count}/{max_batch_retries}")
        time.sleep(BASE_DELAY)  # Wait before retrying
    
    # Debug print
    print(f"Processing batch with {len(relationships)} relationships using timestamp: {article_timestamp}")
    
    # Process the relationships to update the graph and collect new entities
    next_entities = []
    
    for source, impact, target, content in relationships:
        # Skip invalid relationships
        if "kh√¥ng c√≥ th·ª±c th·ªÉ n√†o" in source.lower() or "kh√¥ng c√≥ th·ª±c th·ªÉ n√†o" in target.lower():
            continue
            
        canon_source = source  # Source entity should already be canonical
        canon_target = merge_entity(target, canonical_entities)
        
        # Determine if target is a stock
        node_type = "stock" if any(str(canon_target).lower().find(stock.lower()) != -1 for stock in portfolio) else "entity"
        
        # Add node if it doesn't exist
        if not G.has_node(canon_target):
            G.add_node(canon_target, type=node_type, timestamp=article_timestamp)
            
        # Add edge from source to target, ensuring we use the article timestamp
        add_edge(G, canon_source, canon_target, impact, article_timestamp)
        
        # Add to frontier if it's an entity (not a stock)
        if node_type == "entity":
            next_entities.append((canon_target, impact, content))
    
    return next_entities

# ---------------------------
# Knowledge Graph Construction
# ---------------------------
def process_article(idx, row, G, canonical_entities, portfolio, portfolio_sector, max_frontier_size=15):
    """
    Process a single article to extract entities and build relationships
    Thread-safe function to be used with multithreading
    """
    # Build portfolio string
    portfolio_str_full = ", ".join([f"{stock}-{sector}" for stock, sector in zip(portfolio, portfolio_sector)])
    
    article_node = f"Article_{idx}: {row['title']}"
    # article_timestamp = row['parsed_date']  # Store article timestamp for later use
    article_timestamp = row['date'] 
    
    # Thread-safe add node to graph
    if not G.has_node(article_node):
        G.add_node(article_node, type="article", timestamp=article_timestamp)
    
    # Phase 1: Extract initial entities
    max_entity_retries = 3
    entity_retry_count = 0
    entities_dict = {"POSITIVE": [], "NEGATIVE": []}
    
    while entity_retry_count < max_entity_retries:
        prompt_text = {
            "portfolio": portfolio_str_full,
            "date": row['date'],
            "group": row['group'],
            "title": row['title'],
            "description": row['description'],
            "existing_entities": graph_entities_to_str(G)
        }
        
        response_text = invoke_chain_with_retry(chain_entity, prompt_text)
        time.sleep(1)  # Rate limiting
        
        if response_text is None:
            print(f"Skipping article {idx} due to API errors")
            return 0, 0  # Return zeros for new nodes/edges counts
        
        entities_dict = parse_entity_response(response_text)
        
        # Check if we got any entities
        total_entities = len(entities_dict.get("POSITIVE", [])) + len(entities_dict.get("NEGATIVE", []))
        if total_entities > 0:
            break
            
        entity_retry_count += 1
        print(f"Article {idx} returned 0 entities. Retry {entity_retry_count}/{max_entity_retries}")
        time.sleep(BASE_DELAY)  # Wait before retrying
    
    if entity_retry_count == max_entity_retries and total_entities == 0:
        print(f"Failed to extract entities from article {idx} after {max_entity_retries} attempts")
        return 0, 0
    
    # Process initial entities
    initial_entities = []
    new_nodes = 0
    new_edges = 0
    
    for impact in ["POSITIVE", "NEGATIVE"]:
        for ent, content in entities_dict.get(impact, []):
            # Skip invalid entities
            if not ent or "kh√¥ng c√≥ th·ª±c th·ªÉ n√†o" in ent.lower():
                continue
                
            canon_ent = None
            
            # Thread-safe check for existing entity
            normalized_entity = str(ent).strip('[').strip(']').strip(' ').lower()
            existing = False
            for exist in canonical_entities:
                if exist.lower() == normalized_entity:
                    canon_ent = exist
                    existing = True
                    break
            
            if not existing:
                # Thread-safe update of canonical_entities
                canonical_entities.add(normalized_entity)
                canon_ent = normalized_entity
                
            # Determine node type
            node_type = "stock" if any(str(canon_ent).lower().find(stock.lower()) != -1 for stock in portfolio) else "entity"
            
            # Thread-safe add node to graph
            if not G.has_node(canon_ent):
                G.add_node(canon_ent, type=node_type, timestamp=article_timestamp)
                new_nodes += 1
            
            if node_type == "entity":
                initial_entities.append((canon_ent, impact, content))
                
            # Thread-safe add edge
            if not G.has_edge(article_node, canon_ent):
                G.add_edge(article_node, canon_ent, impact=impact, timestamp=article_timestamp)
                new_edges += 1
    
    print(f"Index: {idx}, initial entities: {len(initial_entities)}")

    # Phase 2: Iterative expansion with single batch per iteration
    frontier = initial_entities
    iter_count = 0
    max_iterations = MAX_ITER  # Maximum frontier expansion iterations
    
    while frontier and iter_count < max_iterations:
        # Limit frontier size to prevent model context overload
        if len(frontier) > max_frontier_size:
            frontier = random.sample(frontier, max_frontier_size)
            print(f"Index: {idx}, limited frontier to {max_frontier_size} entities")
            
        # Process the entire frontier in a single API call
        next_frontier = batch_process_entity_relationships(
            frontier, G, canonical_entities, portfolio, portfolio_str_full, article_timestamp
        )
        
        # Remove duplicates by entity name
        frontier = list({ent: (ent, imp, txt) for ent, imp, txt in next_frontier}.values())
        
        # Limit frontier size again if needed
        if len(frontier) > max_frontier_size:
            frontier = random.sample(frontier, max_frontier_size)
        
        print(f"Index: {idx}, next frontier: {len(frontier)}")
        iter_count += 1
    
    return new_nodes, new_edges

def build_knowledge_graph(df, portfolio, portfolio_sector, skip=0, use_threading=True, max_frontier_size=10, max_workers=5, 
                     graph_checkpoint=None, canonical_checkpoint=None):
    """
    Builds a knowledge graph from articles with optimized batch processing.
    Processes multiple articles in parallel using threads.
    
    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame containing articles to process
    portfolio : list
        List of portfolio stock symbols
    portfolio_sector : list
        List of portfolio sectors
    skip : int, optional
        Number of articles to skip
    use_threading : bool, optional
        Whether to use multithreading for processing
    max_frontier_size : int, optional
        Maximum size of frontier to prevent model context overload
    max_workers : int, optional
        Maximum number of worker threads to use
    graph_checkpoint : str, optional
        Path to knowledge graph checkpoint file to load
    canonical_checkpoint : str, optional
        Path to canonical entities checkpoint file to load
    
    Returns:
    --------
    nx.DiGraph
        The constructed knowledge graph
    """
    # Initialize graph and canonical entities
    G = nx.DiGraph()
    canonical_entities = set()
    
    # Load from checkpoints if provided
    if graph_checkpoint and os.path.exists(graph_checkpoint):
        print(f"Loading knowledge graph from checkpoint: {graph_checkpoint}")
        try:
            with open(graph_checkpoint, "rb") as f:
                G = pickle.load(f)
            print(f"Loaded graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges")
        except Exception as e:
            print(f"Error loading graph checkpoint: {e}")
            print("Starting with empty graph")
    
    if canonical_checkpoint and os.path.exists(canonical_checkpoint):
        print(f"Loading canonical entities from checkpoint: {canonical_checkpoint}")
        try:
            with open(canonical_checkpoint, "rb") as f:
                canonical_entities = pickle.load(f)
            print(f"Loaded {len(canonical_entities)} canonical entities")
        except Exception as e:
            print(f"Error loading canonical entities checkpoint: {e}")
            print("Starting with empty canonical entities set")
    
    # Filter dataframe to skip articles if needed
    if skip > 0:
        print(f"Skipping first {skip} articles")
        df = df.iloc[skip:]
    
    # Define chunk size for processing (for checkpoints)
    chunk_size = 10
    
    # Process in chunks to allow for saving checkpoints
    for chunk_start in range(0, len(df), chunk_size):
        chunk_end = min(chunk_start + chunk_size, len(df))
        chunk_df = df.iloc[chunk_start:chunk_end]
        
        print(f"Processing articles {skip+chunk_start} to {skip+chunk_end-1}")
        
        # Process articles (either in parallel or sequentially)
        if use_threading and len(chunk_df) > 1:
            # Use ThreadPoolExecutor for parallel processing
            articles_processed = 0
            with ThreadPoolExecutor(max_workers=min(max_workers, len(chunk_df))) as executor:
                # Submit tasks
                futures = [
                    executor.submit(
                        process_article, 
                        idx, 
                        row, 
                        G, 
                        canonical_entities, 
                        portfolio, 
                        portfolio_sector, 
                        max_frontier_size
                    ) 
                    for idx, row in chunk_df.iterrows()
                ]
                
                # Process results as they complete
                for future in futures:
                    new_nodes, new_edges = future.result()
                    articles_processed += 1
                    
            print(f"Processed {articles_processed} articles in parallel")
        else:
            # Process sequentially
            for idx, row in chunk_df.iterrows():
                new_nodes, new_edges = process_article(
                    idx, 
                    row, 
                    G, 
                    canonical_entities, 
                    portfolio, 
                    portfolio_sector, 
                    max_frontier_size
                )
        
        # Save checkpoint after each chunk
        checkpoint_file = f"knowledge_graph_p3_checkpoint_{skip+chunk_end}.pkl"
        with open(checkpoint_file, "wb") as f:
            pickle.dump(G, f)
        
        canonical_set_file = f"canonical_set_checkpoint_{skip+chunk_end}.pkl"
        with open(canonical_set_file, "wb") as f:
            pickle.dump(canonical_entities, f)
            
        print(f"Saved checkpoint after processing {skip+chunk_end} articles")
        print(f"Graph now has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges")
    
    # Save final graph
    with open("knowledge_graph_p3.pkl", "wb") as f:
        pickle.dump(G, f)
    with open("canonical_set.pkl", "wb") as f:
        pickle.dump(canonical_entities, f)
    
    print(f"Completed graph building with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges")
    return G

# ---------------------------
# Final Reasoning
# ---------------------------
def final_reasoning(G, portfolio, portfolio_sector, prediction_date):
    """
    Chuy·ªÉn ƒë·ªì th·ªã th√†nh chu·ªói tuple v√† ch·∫°y chu·ªói suy lu·∫≠n cu·ªëi c√πng, truy·ªÅn ng√†y d·ª± ƒëo√°n.
    
    Parameters:
    -----------
    G : nx.DiGraph
        ƒê·ªì th·ªã con G_TRR
    portfolio : list
        Danh s√°ch c·ªï phi·∫øu trong danh m·ª•c
    portfolio_sector : list
        Danh s√°ch lƒ©nh v·ª±c c·ªßa danh m·ª•c
    prediction_date : str
        Ng√†y d·ª± ƒëo√°n ·ªü ƒë·ªãnh d·∫°ng ISO (v√≠ d·ª•: '2025-03-26T01:00:00+07:00')
    
    Returns:
    --------
    Response t·ª´ LLM
    """
    tuples_str = graph_to_tuples(G)
    
    with open("tuples.txt", "w", encoding="utf-8") as f:
        f.write(tuples_str)
    
    portfolio_str_full = ", ".join([f"{stock}-{sector}" for stock, sector in zip(portfolio, portfolio_sector)])

    print("\nTuple ƒë·∫ßu v√†o cho suy lu·∫≠n:")
    print(f"S·ªë c·∫°nh: {G.number_of_edges()}")
    
    reasoning_prompt = {
        "tuples": tuples_str,
        "portfolio": portfolio_str_full,
        "prediction_date": prediction_date
    }
    
    response = invoke_chain_with_retry(chain_reasoning, reasoning_prompt, max_retries=MAX_RETRIES*2)
    time.sleep(2)
    return response

def make_summarized_news(df: pd.DataFrame, batch_size=5):
    """
    Creates a summarized version of the news articles with batched processing
    """
    # Extract date part only for grouping
    df['only_date'] = pd.to_datetime(df['parsed_date']).dt.date
    df_summarized = pd.DataFrame(columns=["postID", "title", "description", "date", "group"])
    
    # Build portfolio string once
    portfolio_str = ", ".join(PORTFOLIO_STOCKS)
    
    # Group by date for processing
    date_groups = df.groupby("only_date")
    total_groups = len(date_groups)
    
    print(f"Processing {total_groups} dates for summarization")
    
    # Process in batches to allow checkpoints
    current_idx = 1
    for batch_idx, batch in enumerate(range(0, total_groups, batch_size)):
        batch_summaries = []
        
        # Process each date in this batch
        for i, (date, group) in enumerate(list(date_groups)[batch:batch+batch_size]):
            # check if date is before 22/03/2025
            # if date <= pd.to_datetime("2025-01-31").date():
            #     print(f"Skipping date {date}")
            #     continue
            # Combine articles for this date
            combined_articles = combine_articles(group)
            
            # Create prompt for summary
            summary_prompt = {
                "articles_list": combined_articles,
                "portfolio": portfolio_str,
            }
            
            print(f"Processing date {batch*batch_size + i + 1}/{total_groups}: {date}, articles: {len(group)}")
            
            # Get summary with retry logic for empty results
            max_summary_retries = 10
            summary_retry_count = 0
            summary_df = pd.DataFrame(columns=["postID", "title", "description", "date", "group"])
            
            while summary_retry_count < max_summary_retries:
                # Get summary with retry logic
                if summary_retry_count < 3:
                    summary_response = invoke_chain_with_retry(chain_summary, summary_prompt)
                    time.sleep(2)
                elif summary_retry_count < 5:
                    summary_response = invoke_chain_with_retry(chain_summary_more_temperature, summary_prompt)
                    time.sleep(2)
                else:
                    summary_response = invoke_chain_with_retry(chain_summary_pro, summary_prompt)
                    time.sleep(2)
                
                time.sleep(1)  # Rate limiting
                
                if summary_response is None:
                    print(f"Failed to summarize articles for date {date}")
                    break
                    
                # Format date for output
                date_str = f"{date}T16:00:00+07:00"
                
                # Parse summary response
                summary_df = parse_summary_response(summary_response, date_str, starting_index=current_idx)
                
                # Check if we got any articles
                if len(summary_df) > 0:
                    break
                    
                summary_retry_count += 1
                print(f"Summary for date {date} returned 0 articles. Retry {summary_retry_count}/{max_summary_retries}")
                time.sleep(BASE_DELAY)  # Wait before retrying
            
            # Only proceed if we have articles
            if len(summary_df) > 0:
                current_idx += len(summary_df)
                # Add to batch results
                batch_summaries.append(summary_df)
            else:
                print(f"‚ö† Failed to get any summarized articles for date {date} after {max_summary_retries} attempts")
        
        # Combine all summaries in this batch
        if batch_summaries:
            batch_df = pd.concat(batch_summaries, ignore_index=True)
            df_summarized = pd.concat([df_summarized, batch_df], ignore_index=True)
            
            # Save checkpoint after each batch
            checkpoint_file = f"summarized_articles_checkpoint_{batch_idx}.csv"
            df_summarized.to_csv(checkpoint_file, index=False)
            print(f"Saved checkpoint with {len(df_summarized)} articles to {checkpoint_file}")
    
    # Save final result
    df_summarized.to_csv("summarized_articles.csv", index=False)
    print(f"Saved {len(df_summarized)} summarized articles to CSV")
    
    return df_summarized

def trr(df, prediction_date, load_saved_graph=False, lambda_decay=1.0, q=6, max_frontier_size=10, 
        use_threading=True, max_workers=5, skip=0, graph_checkpoint=None, canonical_checkpoint=None):
    """
    Main TRR function to build knowledge graph and make predictions
    
    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame containing summarized articles
    prediction_date : str
        Date for which to make predictions
    load_saved_graph : bool, optional
        Whether to load an existing graph or build a new one
    lambda_decay : float, optional
        Decay factor for edge weights
    q : int, optional
        Number of top-ranked entities to include in subgraph
    max_frontier_size : int, optional
        Maximum number of entities to process in a single batch
    use_threading : bool, optional
        Whether to use multithreading for processing
    max_workers : int, optional
        Maximum number of worker threads to use
    skip : int, optional
        Number of articles to skip in processing
    graph_checkpoint : str, optional
        Path to knowledge graph checkpoint file to load
    canonical_checkpoint : str, optional
        Path to canonical entities checkpoint file to load
    
    Returns:
    --------
    The prediction result
    """
    try:
        pred_ts = isoparse(str(prediction_date)).timestamp()
        print(f"Timestamp d·ª± ƒëo√°n: {pred_ts}")
    except Exception as e:
        print(f"L·ªói khi ph√¢n t√≠ch ng√†y d·ª± ƒëo√°n: {e}. S·ª≠ d·ª•ng prediction_date nh∆∞ hi·ªán t·∫°i.")
        pred_ts = prediction_date
    
    if load_saved_graph:
        print("T·∫£i ƒë·ªì th·ªã tri th·ª©c hi·ªán c√≥...")
        try:
            with open(graph_checkpoint or "knowledge_graph_p3_fixed_0227-0407.pkl", "rb") as f:
                G = pickle.load(f)
            print(f"ƒê√£ t·∫£i ƒë·ªì th·ªã v·ªõi {G.number_of_nodes()} ƒë·ªânh v√† {G.number_of_edges()} c·∫°nh")
        except Exception as e:
            print(f"L·ªói khi t·∫£i ƒë·ªì th·ªã: {e}")
            print("X√¢y d·ª±ng ƒë·ªì th·ªã tri th·ª©c m·ªõi...")
            G = build_knowledge_graph(
                df, PORTFOLIO_STOCKS, PORTFOLIO_SECTOR, skip=skip, max_frontier_size=max_frontier_size, 
                use_threading=use_threading, max_workers=max_workers, 
                graph_checkpoint=graph_checkpoint, canonical_checkpoint=canonical_checkpoint
            )
    else:
        print("X√¢y d·ª±ng ƒë·ªì th·ªã tri th·ª©c m·ªõi...")
        G = build_knowledge_graph(
            df, PORTFOLIO_STOCKS, PORTFOLIO_SECTOR, skip=skip, max_frontier_size=max_frontier_size, 
            use_threading=use_threading, max_workers=max_workers, 
            graph_checkpoint=graph_checkpoint, canonical_checkpoint=canonical_checkpoint
        )
    
    print(f"√Åp d·ª•ng giai ƒëo·∫°n ch√∫ √Ω cho ng√†y: {prediction_date}...")
    G_sub = attention_phase(G, current_time=pred_ts, lambda_decay=lambda_decay, q=q)
    
    print("Th·ª±c hi·ªán suy lu·∫≠n cu·ªëi c√πng...")
    prediction = final_reasoning(G_sub, PORTFOLIO_STOCKS, PORTFOLIO_SECTOR, prediction_date)
    
    print("\nD·ª± ƒëo√°n cu·ªëi c√πng:")
    print(prediction.content if prediction else "Kh√¥ng c√≥ d·ª± ƒëo√°n")
    
    return prediction

def evaluate_date_range(start_date, end_date, lambda_decay=1, q=6, graph_checkpoint=None, canonical_checkpoint=None):
    """
    Evaluates crash predictions for each day in a date range
    
    Parameters:
    -----------
    start_date : str
        Start date for evaluation in ISO format
    end_date : str
        End date for evaluation in ISO format
    lambda_decay : float
        Decay factor for edge weights
    q : int
        Number of top-ranked entities to include in subgraph
    graph_checkpoint : str
        Path to knowledge graph checkpoint file to load
    canonical_checkpoint : str
        Path to canonical entities checkpoint file to load
    """
    # Convert to datetime objects for iteration
    start_dt = pd.to_datetime(start_date)
    end_dt = pd.to_datetime(end_date)
    
    # Load the graph once
    if not graph_checkpoint:
        print("Error: Graph checkpoint required for evaluation mode")
        return
        
    print(f"Loading knowledge graph from {graph_checkpoint}...")
    try:
        with open(graph_checkpoint, "rb") as f:
            G = pickle.load(f)
        print(f"Loaded graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges")
    except Exception as e:
        print(f"Error loading graph: {e}")
        return
        
    # Set up results storage
    results = []
    results_file = f"crash_predictions_{start_dt.date()}_to_{end_dt.date()}.csv"
    
    # Check if we're continuing an existing evaluation
    if os.path.exists(results_file):
        existing_results = pd.read_csv(results_file)
        # Get the last evaluated date
        if len(existing_results) > 0:
            last_date = pd.to_datetime(existing_results.iloc[-1]['prediction_date'])
            start_dt = last_date + pd.Timedelta(days=1)
            print(f"Continuing evaluation from {start_dt.date()}")
            results = existing_results.to_dict('records')
    
    # Iterate through each day in the range
    current_dt = start_dt
    while current_dt <= end_dt:
        # Set prediction time to 1 AM GMT+7
        prediction_time = current_dt.replace(hour=1, minute=0, second=0)
        prediction_date = prediction_time.isoformat()
        
        print(f"\n{'='*50}")
        print(f"Evaluating prediction for {prediction_date}")
        print(f"{'='*50}")
        
        # Convert to timestamp for evaluation
        pred_ts = prediction_time.timestamp()
        
        # Apply attention phase
        print("Applying attention phase (PageRank-based filtering)...")
        G_sub = attention_phase(G, current_time=pred_ts, lambda_decay=1.0, q=6)
        print(f"Created subgraph with {G_sub.number_of_nodes()} nodes and {G_sub.number_of_edges()} edges")
        
        # Final reasoning
        print("Running final reasoning...")
        prediction = final_reasoning(G_sub, PORTFOLIO_STOCKS, PORTFOLIO_SECTOR)
        
        if prediction:
            # Parse the prediction result
            response_text = prediction.content.strip().lower()
            print("\nPrediction Response:")
            print(response_text)
            
            # Simple parsing: check if 'yes' appears before 'no'
            yes_pos = response_text.find('yes')
            no_pos = response_text.find('no')
            
            # Determine crash prediction
            if yes_pos != -1 and (no_pos == -1 or yes_pos < no_pos):
                crash_prediction = "Yes"
            elif no_pos != -1:
                crash_prediction = "No"
            else:
                crash_prediction = "Unclear"
            
            # Record the result
            result = {
                'prediction_date': prediction_date,
                'crash_prediction': crash_prediction,
                'full_response': prediction.content
            }
            
            results.append(result)
            
            # Save to CSV after each prediction for backup
            pd.DataFrame(results).to_csv(results_file, index=False)
            print(f"Saved prediction to {results_file} - Prediction: {crash_prediction}")
        else:
            print("Error: No prediction available")
            # Still record the failure
            result = {
                'prediction_date': prediction_date,
                'crash_prediction': "Error",
                'full_response': ""
            }
            results.append(result)
            pd.DataFrame(results).to_csv(results_file, index=False)
            
        # Move to next day
        # ignore saturday and sunday
        current_dt += pd.Timedelta(days=1)
        while current_dt.weekday() >= 5:
            current_dt += pd.Timedelta(days=1)
            continue
        
    print(f"\nEvaluation complete. Processed {len(results)} days.")
    return results

def main():
    """
    ƒêi·ªÉm v√†o ch√≠nh v·ªõi x·ª≠ l√Ω tham s·ªë c·∫£i ti·∫øn, h·ªó tr·ª£ d·ª± ƒëo√°n cho chu·ªói ng√†y v√† l∆∞u d·ª± ƒëo√°n v√†o cash_prediction.txt
    """
    parser = argparse.ArgumentParser(description="M√¥ h√¨nh Temporal Relational Reasoning")
    parser.add_argument("--news_from", type=int, default=1, help="Ch·ªâ s·ªë b·∫Øt ƒë·∫ßu c·ªßa b√†i b√°o")
    parser.add_argument("--news_to", type=int, default=9400, help="Ch·ªâ s·ªë k·∫øt th√∫c c·ªßa b√†i b√°o (b·ªã b·ªè qua n·∫øu d√πng pred_date_range)")
    parser.add_argument("--pred_date", type=str, default="2025-04-02T01:00:00+07:00", help="Ng√†y d·ª± ƒëo√°n (d√πng n·∫øu kh√¥ng c√≥ pred_date_range)")
    parser.add_argument("--pred_date_range", type=str, help="Kho·∫£ng ng√†y d·ª± ƒëo√°n (ƒë·ªãnh d·∫°ng: start_date,end_date, v√≠ d·ª•: 2025-03-26,2025-03-31)")
    parser.add_argument("--summarize", action="store_true", help="T·∫°o tin t·ª©c t√≥m t·∫Øt")
    parser.add_argument("--load_graph", action="store_true", help="T·∫£i ƒë·ªì th·ªã tri th·ª©c hi·ªán c√≥")
    parser.add_argument("--lambda_decay", type=float, default=1.0, help="Tham s·ªë suy gi·∫£m lambda")
    parser.add_argument("--q", type=int, default=6, help="S·ªë th·ª±c th·ªÉ top-q ƒë∆∞·ª£c ch·ªçn")
    parser.add_argument("--max_frontier_size", type=int, default=10, help="S·ªë th·ª±c th·ªÉ t·ªëi ƒëa x·ª≠ l√Ω trong m·ªôt l√¥")
    parser.add_argument("--batch_size", type=int, default=5, help="K√≠ch th∆∞·ªõc l√¥ cho t√≥m t·∫Øt tin t·ª©c")
    parser.add_argument("--no_threading", action="store_true", help="T·∫Øt ƒëa lu·ªìng")
    parser.add_argument("--max_workers", type=int, default=5, help="S·ªë lu·ªìng c√¥ng nh√¢n t·ªëi ƒëa")
    parser.add_argument("--skip", type=int, default=0, help="S·ªë b√†i b√°o c·∫ßn b·ªè qua trong x·ª≠ l√Ω")
    parser.add_argument("--graph_checkpoint", type=str, help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn file checkpoint c·ªßa ƒë·ªì th·ªã tri th·ª©c")
    parser.add_argument("--canonical_checkpoint", type=str, help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn file checkpoint c·ªßa t·∫≠p th·ª±c th·ªÉ chu·∫©n h√≥a")
    
    args = parser.parse_args()
    
    # ƒê·ªçc d·ªØ li·ªáu tin t·ª©c
    print(f"ƒê·ªçc d·ªØ li·ªáu tin t·ª©c t·ª´ ch·ªâ s·ªë {args.news_from}...")
    df = read_news_data("cleaned_posts.csv")
    
    # L·ªçc v√† ti·ªÅn x·ª≠ l√Ω
    print("Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu tin t·ª©c...")
    df = df[df['group'] != "Doanh nghi·ªáp"]
    df = df.iloc[::-1]  # ƒê·∫£o ng∆∞·ª£c ƒë·ªÉ theo th·ª© t·ª± th·ªùi gian
    df.fillna("", inplace=True)
    
    # T·∫°o ho·∫∑c t·∫£i tin t·ª©c t√≥m t·∫Øt
    if args.summarize:
        print("T·∫°o tin t·ª©c t√≥m t·∫Øt...")
        df_summary = make_summarized_news(df, batch_size=args.batch_size)
    else:
        if not os.path.exists("summarized_articles.csv"):
            print("L·ªói: File summarized_articles.csv kh√¥ng t·ªìn t·∫°i. Vui l√≤ng ch·∫°y v·ªõi --summarize ho·∫∑c cung c·∫•p file.")
            return
        print("T·∫£i tin t·ª©c t√≥m t·∫Øt hi·ªán c√≥...")
        df_summary = pd.read_csv("summarized_articles.csv")
    
    # File ƒë·ªÉ l∆∞u d·ª± ƒëo√°n
    prediction_file = "crash_prediction.txt"
    
    # Ch·∫ø ƒë·ªô d·ª± ƒëo√°n cho chu·ªói ng√†y
    if args.pred_date_range:
        try:
            start_date, end_date = args.pred_date_range.split(',')
            print(f"Ch·∫°y d·ª± ƒëo√°n cho kho·∫£ng ng√†y t·ª´ {start_date} ƒë·∫øn {end_date}")
            
            start_dt = pd.to_datetime(start_date)
            end_dt = pd.to_datetime(end_date)
            results = []
            
            current_dt = start_dt
            news_idx = args.news_from
            while current_dt <= end_dt:
                if news_idx + ARTICLES_PER_DATE - 1 >= len(df_summary):
                    print(f"H·∫øt b√†i b√°o t·∫°i ch·ªâ s·ªë {news_idx} cho ng√†y {current_dt.date()}")
                    break
                
                prediction_time = current_dt.replace(hour=1, minute=0, second=0)
                prediction_date = prediction_time.isoformat()
                
                print(f"\n{'='*50}")
                print(f"D·ª± ƒëo√°n cho ng√†y {prediction_date}")
                print(f"S·ª≠ d·ª•ng b√†i b√°o t·ª´ {news_idx} ƒë·∫øn {news_idx + ARTICLES_PER_DATE - 1}")
                print(f"{'='*50}")
                
                df_subset = df_summary.iloc[news_idx:news_idx + ARTICLES_PER_DATE]
                if len(df_subset) < ARTICLES_PER_DATE:
                    print(f"L·ªói: Kh√¥ng ƒë·ªß {ARTICLES_PER_DATE} b√†i b√°o t·ª´ ch·ªâ s·ªë {news_idx}")
                    break
                
                prediction = trr(
                    df_subset, 
                    prediction_date,
                    load_saved_graph=args.load_graph,
                    lambda_decay=args.lambda_decay,
                    q=args.q,
                    max_frontier_size=args.max_frontier_size,
                    use_threading=not args.no_threading,
                    max_workers=args.max_workers,
                    skip=args.skip,
                    graph_checkpoint=args.graph_checkpoint,
                    canonical_checkpoint=args.canonical_checkpoint
                )
                
                # Ghi d·ª± ƒëo√°n v√†o cash_prediction.txt
                with open(prediction_file, "a", encoding="utf-8") as pred_f:
                    pred_f.write(f"Prediction for {prediction_date}:\n")
                    pred_f.write(f"{prediction.content if prediction else 'Error: No prediction available'}\n\n")
                    pred_f.flush()  # ƒê·∫£m b·∫£o ghi ngay l·∫≠p t·ª©c
                
                print("\nPh·∫£n h·ªìi d·ª± ƒëo√°n:")
                response_text = prediction.content.strip().lower() if prediction else ""
                print(response_text)
                
                # L∆∞u k·∫øt qu·∫£ cho CSV
                crash_prediction = "Error"
                if prediction:
                    yes_pos = response_text.find('yes')
                    no_pos = response_text.find('no')
                    crash_prediction = "Yes" if yes_pos != -1 and (no_pos == -1 or yes_pos < no_pos) else "No" if no_pos != -1 else "Unclear"
                
                result = {
                    'prediction_date': prediction_date,
                    'news_indices': f"{news_idx}-{news_idx + ARTICLES_PER_DATE - 1}",
                    'crash_prediction': crash_prediction,
                    'full_response': prediction.content if prediction else ""
                }
                results.append(result)
                
                # L∆∞u v√†o CSV
                results_file = f"crash_predictions_{start_dt.date()}_to_{end_dt.date()}.csv"
                pd.DataFrame(results).to_csv(results_file, index=False)
                print(f"L∆∞u d·ª± ƒëo√°n v√†o {results_file} - D·ª± ƒëo√°n: {crash_prediction}")
                print(f"L∆∞u d·ª± ƒëo√°n v√†o {prediction_file}")
                
                # Chuy·ªÉn sang ng√†y ti·∫øp theo (b·ªè qua th·ª© B·∫£y v√† Ch·ªß Nh·∫≠t)
                current_dt += pd.Timedelta(days=1)
                while current_dt.weekday() >= 5:
                    current_dt += pd.Timedelta(days=1)
                news_idx += ARTICLES_PER_DATE
            
            print(f"\nHo√†n th√†nh ƒë√°nh gi√°. ƒê√£ x·ª≠ l√Ω {len(results)} ng√†y")
            return results
            
        except ValueError:
            print("L·ªói: pred_date_range ph·∫£i c√≥ ƒë·ªãnh d·∫°ng 'start_date,end_date' (v√≠ d·ª•: '2025-03-26,2025-03-31')")
            return
    
    # Ch·∫ø ƒë·ªô d·ª± ƒëo√°n ƒë∆°n ng√†y
    print(f"Ch·∫°y TRR cho ng√†y d·ª± ƒëo√°n: {args.pred_date}")
    news_to = args.news_from + ARTICLES_PER_DATE - 1
    df_subset = df_summary.iloc[args.news_from:news_to + 1]
    if len(df_subset) < ARTICLES_PER_DATE:
        print(f"L·ªói: Kh√¥ng ƒë·ªß {ARTICLES_PER_DATE} b√†i b√°o t·ª´ ch·ªâ s·ªë {args.news_from}")
        return
    
    prediction = trr(
        df_subset, 
        args.pred_date,
        load_saved_graph=args.load_graph,
        lambda_decay=args.lambda_decay,
        q=args.q,
        max_frontier_size=args.max_frontier_size,
        use_threading=not args.no_threading,
        max_workers=args.max_workers,
        skip=args.skip,
        graph_checkpoint=args.graph_checkpoint,
        canonical_checkpoint=args.canonical_checkpoint
    )
    
    # Ghi d·ª± ƒëo√°n ƒë∆°n ng√†y v√†o cash_prediction.txt
    with open(prediction_file, "a", encoding="utf-8") as pred_f:
        pred_f.write(f"Prediction for {args.pred_date}:\n")
        pred_f.write(f"{prediction.content if prediction else 'Error: No prediction available'}\n\n")
        pred_f.flush()
        print(f"L∆∞u d·ª± ƒëo√°n v√†o {prediction_file}")
    
    return prediction
if __name__ == "__main__":
    main()
    